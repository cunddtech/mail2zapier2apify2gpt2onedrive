#!/usr/bin/env python3
"""
üöÄ PRODUCTION-READY AI COMMUNICATION ORCHESTRATOR
=================================================

LANGGRAPH/LANGCHAIN MIGRATION - SOFORT LAUFF√ÑHIG!

SYSTEM FEATURES:
‚úÖ LangGraph State Management
‚úÖ OpenAI GPT-4 Integration
‚úÖ Contact Matching (Apify/WeClapp)
‚úÖ WEG A/B Workflow Routing
‚úÖ Email/Call/WhatsApp Processing
‚úÖ FastAPI Production Server
‚úÖ Docker Container Ready
‚úÖ QNAP Container Station Compatible

PRODUCTION URLs:
üåê http://192.168.0.101:5001 (QNAP Local)
üîó https://qlink.to/CundD:5001 (Public Smart URL)

DEPLOYMENT: Kopieren ‚Üí Docker Build ‚Üí Container Station Deploy
"""

import os
import json
import asyncio
from datetime import datetime, timedelta
from typing import Dict, List, Any, Optional, TypedDict
from dataclasses import dataclass
import pytz  # Timezone support

# Deutsche Zeitzone
BERLIN_TZ = pytz.timezone('Europe/Berlin')

def now_berlin():
    """Return current datetime in Berlin timezone"""
    return datetime.now(BERLIN_TZ)

# LangGraph/LangChain Imports
from langgraph.graph import StateGraph, END
from langchain_openai import ChatOpenAI
from langchain_core.messages import HumanMessage, AIMessage
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import JsonOutputParser

# FastAPI Production Server
from fastapi import FastAPI, HTTPException, Request
from fastapi.responses import Response as FastAPIResponse, JSONResponse
from fastapi.middleware.cors import CORSMiddleware
import uvicorn

# HTTP Client f√ºr API Calls
import aiohttp
import httpx
import logging

# SQLite Database f√ºr Contact Cache
import sqlite3
import asyncio
from contextlib import asynccontextmanager
import requests

# üí∞ Richtpreis-Berechnung f√ºr Anrufe
from modules.pricing.estimate_from_call import (
    calculate_estimate_from_transcript,
    format_estimate_for_email,
    ProjectEstimate
)

# üìÇ Apify Module Imports - Ordnerstruktur & Dokumenten-Klassifikation
from modules.filegen.folder_logic import generate_folder_and_filenames
from modules.gpt.classify_document_with_gpt import classify_document_with_gpt

# üóÑÔ∏è Email Tracking Database - Duplikatpr√ºfung
from modules.database.email_tracking_db import get_email_tracking_db, EmailTrackingDB

# ‚òÅÔ∏è OneDrive Upload
from modules.upload.upload_file_to_onedrive import upload_file_to_onedrive

# INLINE Graph API Functions (Railway deployment workaround)
async def get_graph_token_mail():
    """Holt das Zugriffstoken von Microsoft Graph f√ºr Mail."""
    tenant_id = os.getenv("GRAPH_TENANT_ID_MAIL")
    client_id = os.getenv("GRAPH_CLIENT_ID_MAIL")
    client_secret = os.getenv("GRAPH_CLIENT_SECRET_MAIL")
    
    if not tenant_id or not client_id or not client_secret:
        logger.error("‚ùå Fehlende Mail-Graph API Zugangsdaten")
        return None
    
    url = f"https://login.microsoftonline.com/{tenant_id}/oauth2/v2.0/token"
    headers = {"Content-Type": "application/x-www-form-urlencoded"}
    data = {
        "grant_type": "client_credentials",
        "client_id": client_id,
        "client_secret": client_secret,
        "scope": "https://graph.microsoft.com/.default"
    }
    
    try:
        logger.info(f"üîê Requesting token: tenant={tenant_id[:8]}..., client={client_id[:8]}...")
        async with httpx.AsyncClient() as client:
            response = await client.post(url, headers=headers, data=data)
            if response.status_code == 200:
                logger.info("‚úÖ Graph API token obtained successfully")
                return response.json().get("access_token")
            else:
                error_body = response.text
                logger.error(f"‚ùå Token error {response.status_code}: {error_body}")
                return None
    except Exception as e:
        logger.error(f"‚ùå Token exception: {e}")
        import traceback
        logger.error(traceback.format_exc())
        return None

async def fetch_email_details_with_attachments(user_email, message_id, access_token):
    """Ruft die E-Mail-Daten und Anh√§nge von Microsoft Graph ab."""
    email_url = f"https://graph.microsoft.com/v1.0/users/{user_email}/messages/{message_id}?$expand=attachments"
    headers = {
        "Authorization": f"Bearer {access_token}",
        "Accept": "application/json"
    }
    
    logger.info(f"üîç Loading email: user={user_email}, message={message_id[:20]}...")
    try:
        response = requests.get(email_url, headers=headers)
        if response.status_code == 200:
            email_data = response.json()
            logger.info(f"‚úÖ Email loaded: Subject='{email_data.get('subject', 'no subject')}', From='{email_data.get('from', {}).get('emailAddress', {}).get('address', 'unknown')}'")
            return email_data
        else:
            logger.error(f"‚ùå Graph API error: {response.status_code} - {response.text[:200]}")
            return None
    except Exception as e:
        logger.error(f"‚ùå Email fetch exception: {e}")
        return None

# Call Analysis (Task-Ableitung, Termin-Extraktion, Follow-Ups)
from modules.gpt.analyze_call_content import (
    analyze_call_content,
    extract_appointment_datetime,
    calculate_follow_up_date
)

# Logging Setup
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# ===============================
# ZAPIER NOTIFICATION FUNCTIONS
# ===============================

def generate_notification_html(notification_data: Dict[str, Any]) -> str:
    """
    üé® Generate complete HTML for email notifications
    Supports both WEG A (unknown) and WEG B (known) contacts
    """
    notification_type = notification_data.get("notification_type", "unknown_contact_action_required")
    
    if notification_type == "unknown_contact_action_required":
        # WEG A: Unknown Contact with Action Buttons
        sender = notification_data.get("sender", "Unbekannt")
        sender_display = notification_data.get("sender_display", sender)
        subject = notification_data.get("subject", "Neue Nachricht")
        body_preview = notification_data.get("body_preview", "")
        received_time = notification_data.get("received_time", "")
        ai_analysis = notification_data.get("ai_analysis", {})
        action_options = notification_data.get("action_options", [])
        email_id = notification_data.get("email_id", "unknown")
        webhook_url = notification_data.get("webhook_reply_url", "")
        
        # üí∞ Build price estimate HTML if present (for calls)
        price_estimate = notification_data.get("price_estimate")
        has_price_estimate = notification_data.get("has_price_estimate", False)
        price_estimate_html = ""
        
        if has_price_estimate and price_estimate:
            # Use the formatting function from the pricing module
            from modules.pricing.estimate_from_call import ProjectEstimate
            
            # Convert dict back to ProjectEstimate object
            estimate_obj = ProjectEstimate(
                found=True,
                project_type=price_estimate.get("project_type", ""),
                area_sqm=price_estimate.get("area_sqm"),
                material=price_estimate.get("material", ""),
                work_type=price_estimate.get("work_type", ""),
                material_cost=price_estimate.get("material_cost", 0.0),
                labor_cost=price_estimate.get("labor_cost", 0.0),
                additional_cost=price_estimate.get("additional_cost", 0.0),
                total_cost=price_estimate.get("total_cost", 0.0),
                confidence=price_estimate.get("confidence", 0.0),
                calculation_basis=price_estimate.get("calculation_basis", []),
                additional_services=price_estimate.get("additional_services", []),
                notes=price_estimate.get("notes", "")
            )
            
            price_estimate_html = format_estimate_for_email(estimate_obj)
            logger.info(f"‚úÖ Generated price estimate HTML for notification")
        
        # Build attachments HTML if present
        attachments_count = notification_data.get("attachments_count", 0)
        attachment_results = notification_data.get("attachment_results", [])
        logger.info(f"üîç DEBUG generate_notification_html: attachments_count={attachments_count}, attachment_results={len(attachment_results)}")
        
        if attachments_count > 0:
            # Build detailed attachment list with OCR results
            attachment_details = []
            for result in attachment_results:
                name = result.get("filename", "Unbekannt")
                size = result.get("size", 0)
                doc_type = result.get("document_type", "unbekannt")
                ocr_route = result.get("ocr_route", "none")
                ocr_text = result.get("ocr_text", "")
                structured_data = result.get("structured_data", {})
                
                # Format size
                if size > 1024 * 1024:
                    size_str = f"{size / (1024 * 1024):.1f} MB"
                elif size > 1024:
                    size_str = f"{size / 1024:.1f} KB"
                else:
                    size_str = f"{size} B"
                
                # Build detail line
                detail_line = f"üìÑ <strong>{name}</strong> ({size_str})"
                detail_line += f"<br>&nbsp;&nbsp;&nbsp;&nbsp;üè∑Ô∏è Typ: {doc_type}"
                detail_line += f"<br>&nbsp;&nbsp;&nbsp;&nbsp;üîç Verarbeitung: {ocr_route}"
                
                # Add OCR preview if available
                if ocr_text and ocr_text != f"[OCR Placeholder for {name} - Type: {doc_type}]":
                    preview = ocr_text[:150] + "..." if len(ocr_text) > 150 else ocr_text
                    detail_line += f"<br>&nbsp;&nbsp;&nbsp;&nbsp;üìù Inhalt: {preview}"
                
                # Add key structured data if available
                if structured_data:
                    if structured_data.get("invoice_number"):
                        detail_line += f"<br>&nbsp;&nbsp;&nbsp;&nbsp;üî¢ Rechnungs-Nr: {structured_data['invoice_number']}"
                    if structured_data.get("total_amount"):
                        detail_line += f"<br>&nbsp;&nbsp;&nbsp;&nbsp;üí∞ Betrag: {structured_data['total_amount']}"
                    if structured_data.get("vendor_name"):
                        detail_line += f"<br>&nbsp;&nbsp;&nbsp;&nbsp;üè¢ Lieferant: {structured_data['vendor_name']}"
                
                attachment_details.append(detail_line)
            
            if attachment_details:
                details_html = "<br><br>".join(attachment_details)
                attachments_html = f'<p><strong>üìé Anh√§nge ({attachments_count}):</strong><br><br>{details_html}</p>'
            else:
                attachments_html = f'<p><strong>üìé Anh√§nge:</strong> {attachments_count} Datei(en)</p>'
            
            logger.info(f"‚úÖ Generated attachments_html with OCR details: {len(attachment_results)} files")
        else:
            attachments_html = ''
            logger.info(f"‚ö†Ô∏è No attachments, attachments_html is empty")
        
        # Build action buttons HTML
        buttons_html = ""
        for option in action_options:
            action = option.get("action", "")
            label = option.get("label", "")
            description = option.get("description", "")
            color_class = {
                "create_contact": "btn-create",
                "create_supplier": "btn-supplier",
                "add_to_existing": "btn-primary",
                "mark_private": "btn-private",
                "mark_spam": "btn-spam",
                "request_info": "btn-info",
                "data_good": "btn-success",
                "data_error": "btn-warning",
                "report_issue": "btn-secondary"
            }.get(action, "btn-default")
            
            # Route feedback actions to /webhook/feedback, others to /webhook/contact-action
            if action in ["data_good", "data_error", "report_issue"]:
                button_url = f"https://my-langgraph-agent-production.up.railway.app/webhook/feedback?action={action}&sender={sender}&email_id={email_id}"
            else:
                button_url = f"{webhook_url}?action={action}&sender={sender}&email_id={email_id}"
            
            buttons_html += f"""
            <a href="{button_url}" class="button {color_class}">{label}</a>
            <p class="button-desc">{description}</p>
            """
        
        html = f"""
<!DOCTYPE html>
<html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<style>
    body {{ font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif; line-height: 1.6; color: #2C3E50; background: #FFFFFF; margin: 0; padding: 0; }}
    .container {{ max-width: 650px; margin: 0 auto; padding: 0; background: white; border-radius: 15px; box-shadow: 0 8px 32px rgba(0,0,0,0.1); overflow: hidden; }}
    .header {{ background: linear-gradient(135deg, #4ECDC4 0%, #44A08D 100%); color: white; padding: 30px; text-align: center; }}
    .header h2 {{ margin: 0; font-size: 24px; text-shadow: 0 2px 4px rgba(0,0,0,0.1); }}
    .content {{ background: #FFFFFF; padding: 30px; }}
    .info-box {{ background: linear-gradient(135deg, #FFF9E6 0%, #FFE8CC 100%); padding: 20px; margin: 20px 0; border-radius: 10px; border-left: 5px solid #FFB84D; box-shadow: 0 2px 8px rgba(0,0,0,0.05); }}
    .info-box h3 {{ color: #E67E22; margin-top: 0; font-size: 18px; }}
    .info-box p {{ color: #34495E; margin: 8px 0; }}
    .action-buttons {{ margin: 30px 0; text-align: center; }}
    .action-buttons h3 {{ color: #2C3E50; margin-bottom: 20px; }}
    .button {{ display: inline-block; padding: 14px 28px; margin: 8px 5px; text-decoration: none; border-radius: 25px; font-weight: bold; text-align: center; transition: all 0.3s ease; box-shadow: 0 4px 12px rgba(0,0,0,0.15); }}
    .btn-create {{ background: linear-gradient(135deg, #52C234 0%, #47A025 100%); color: white; }}
    .btn-create:hover {{ box-shadow: 0 6px 16px rgba(82,194,52,0.4); transform: translateY(-2px); }}
    .btn-supplier {{ background: linear-gradient(135deg, #FD79A8 0%, #E84393 100%); color: white; }}
    .btn-supplier:hover {{ box-shadow: 0 6px 16px rgba(253,121,168,0.4); transform: translateY(-2px); }}
    .btn-primary {{ background: linear-gradient(135deg, #3498DB 0%, #2980B9 100%); color: white; }}
    .btn-primary:hover {{ box-shadow: 0 6px 16px rgba(52,152,219,0.4); transform: translateY(-2px); }}
    .btn-private {{ background: linear-gradient(135deg, #A29BFE 0%, #6C5CE7 100%); color: white; }}
    .btn-private:hover {{ box-shadow: 0 6px 16px rgba(162,155,254,0.4); transform: translateY(-2px); }}
    .btn-spam {{ background: linear-gradient(135deg, #FF7675 0%, #D63031 100%); color: white; }}
    .btn-spam:hover {{ box-shadow: 0 6px 16px rgba(255,118,117,0.4); transform: translateY(-2px); }}
    .btn-info {{ background: linear-gradient(135deg, #74B9FF 0%, #0984E3 100%); color: white; }}
    .btn-info:hover {{ box-shadow: 0 6px 16px rgba(116,185,255,0.4); transform: translateY(-2px); }}
    .btn-success {{ background: linear-gradient(135deg, #00D2A0 0%, #00B894 100%); color: white; }}
    .btn-success:hover {{ box-shadow: 0 6px 16px rgba(0,210,160,0.4); transform: translateY(-2px); }}
    .btn-warning {{ background: linear-gradient(135deg, #FDCB6E 0%, #E17055 100%); color: white; }}
    .btn-warning:hover {{ box-shadow: 0 6px 16px rgba(253,203,110,0.4); transform: translateY(-2px); }}
    .btn-secondary {{ background: linear-gradient(135deg, #FFEAA7 0%, #FDCB6E 100%); color: #2C3E50; }}
    .btn-secondary:hover {{ box-shadow: 0 6px 16px rgba(255,234,167,0.4); transform: translateY(-2px); }}
    .ai-analysis {{ background: linear-gradient(135deg, #DFE6E9 0%, #B2BEC3 100%); padding: 20px; border-radius: 10px; margin: 20px 0; border-left: 5px solid #74B9FF; }}
    .ai-analysis h3 {{ color: #0984E3; margin-top: 0; font-size: 18px; }}
    .ai-analysis p {{ color: #2C3E50; margin: 8px 0; }}
    .footer {{ text-align: center; padding: 20px; background: linear-gradient(135deg, #F8F9FA 0%, #E9ECEF 100%); color: #7F8C8D; font-size: 13px; border-radius: 0 0 15px 15px; }}
    .button-desc {{ font-size: 12px; margin: 5px 0 15px 0; color: #7F8C8D; }}
    strong {{ color: #2C3E50; }}
</style>
</head>
<body>
<div class="container">
<div class="header">
<h2>ÔøΩÔ∏è Unbekannter Kontakt - Aktion erforderlich</h2>
</div>
<div class="content">
<div class="info-box">
<h3>üìß Details:</h3>
<p><strong>Von:</strong> {sender_display}</p>
<p><strong>Betreff:</strong> {subject}</p>
<p><strong>Empfangen:</strong> {received_time}</p>
{attachments_html}
</div>
<div class="info-box">
<h3>üìù Nachricht:</h3>
<p>{body_preview}</p>
</div>
<div class="ai-analysis">
<h3>ü§ñ KI-Analyse:</h3>
<p><strong>Absicht:</strong> {ai_analysis.get('intent', 'unbekannt')}</p>
<p><strong>Dringlichkeit:</strong> {ai_analysis.get('urgency', 'unbekannt')}</p>
<p><strong>Stimmung:</strong> {ai_analysis.get('sentiment', 'unbekannt')}</p>
{attachments_html}
</div>
{price_estimate_html}
<div class="action-buttons">
<h3>üëÜ W√§hle eine Aktion:</h3>
{buttons_html}
</div>
</div>
<div class="footer">
<p>ü§ñ Automatisch generiert vom C&D Lead Management System</p>
<p>Email-ID: {email_id}</p>
</div>
</div>
</body>
</html>
"""
        # üîç DEBUG: Check if attachments_html is in the generated HTML
        if "Anh√§nge:" in html:
            logger.info(f"‚úÖ HTML contains attachments line!")
        else:
            logger.warning(f"‚ùå HTML does NOT contain attachments line! attachments_html was: {attachments_html}")
        
        return html
    
    else:
        # WEG B: Known Contact - Simple notification with feedback button
        subject = notification_data.get('subject', 'Kontakt verarbeitet')
        summary = notification_data.get('summary', '')
        contact_match = notification_data.get('contact_match', {})
        contact_name = contact_match.get('contact_name', 'Unbekannt')
        contact_id = contact_match.get('contact_id', '')
        from_contact = notification_data.get('from', '')
        content_preview = notification_data.get('content_preview', '')
        ai_analysis = notification_data.get('ai_analysis', {})
        action_options = notification_data.get('action_options', [])
        
        # Build action buttons HTML
        buttons_html = ""
        for option in action_options:
            action = option.get("action", "")
            label = option.get("label", "")
            description = option.get("description", "")
            url = option.get("url", "")
            
            color_class = {
                "view_in_crm": "btn-info",
                "data_good": "btn-success",
                "data_error": "btn-warning",
                "report_issue": "btn-secondary"
            }.get(action, "btn-default")
            
            # Route feedback actions to /webhook/feedback
            if action in ["data_good", "data_error", "report_issue"]:
                button_url = f"https://my-langgraph-agent-production.up.railway.app/webhook/feedback?action={action}&contact_id={contact_id}&from={from_contact}"
            elif url:
                button_url = url
            else:
                button_url = f"https://my-langgraph-agent-production.up.railway.app/webhook/feedback?type=wrong_match&contact_id={contact_id}&from={from_contact}"
            
            buttons_html += f"""
            <a href="{button_url}" class="button {color_class}">{label}</a>
            <p class="button-desc">{description}</p>
            """
        
        return f"""
<!DOCTYPE html>
<html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<style>
    body {{ font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif; line-height: 1.6; color: #2C3E50; background: #FFFFFF; margin: 0; padding: 0; }}
    .container {{ max-width: 650px; margin: 0 auto; padding: 0; background: white; border-radius: 15px; box-shadow: 0 8px 32px rgba(0,0,0,0.1); overflow: hidden; }}
    .header {{ background: linear-gradient(135deg, #55EFC4 0%, #00B894 100%); color: white; padding: 30px; text-align: center; }}
    .header h2 {{ margin: 0; font-size: 24px; text-shadow: 0 2px 4px rgba(0,0,0,0.1); }}
    .content {{ background: #FFFFFF; padding: 30px; }}
    .info-box {{ background: linear-gradient(135deg, #E8F8F5 0%, #D1F2EB 100%); padding: 20px; margin: 20px 0; border-radius: 10px; border-left: 5px solid #00B894; box-shadow: 0 2px 8px rgba(0,0,0,0.05); }}
    .info-box h3 {{ color: #00B894; margin-top: 0; font-size: 18px; }}
    .info-box p {{ color: #34495E; margin: 8px 0; }}
    .action-buttons {{ margin: 30px 0; text-align: center; }}
    .action-buttons h3 {{ color: #2C3E50; margin-bottom: 20px; }}
    .button {{ display: inline-block; padding: 14px 28px; margin: 8px 5px; text-decoration: none; border-radius: 25px; font-weight: bold; text-align: center; transition: all 0.3s ease; box-shadow: 0 4px 12px rgba(0,0,0,0.15); }}
    .btn-info {{ background: linear-gradient(135deg, #74B9FF 0%, #0984E3 100%); color: white; }}
    .btn-info:hover {{ box-shadow: 0 6px 16px rgba(116,185,255,0.4); transform: translateY(-2px); }}
    .btn-success {{ background: linear-gradient(135deg, #00D2A0 0%, #00B894 100%); color: white; }}
    .btn-success:hover {{ box-shadow: 0 6px 16px rgba(0,210,160,0.4); transform: translateY(-2px); }}
    .btn-warning {{ background: linear-gradient(135deg, #FDCB6E 0%, #E17055 100%); color: white; }}
    .btn-warning:hover {{ box-shadow: 0 6px 16px rgba(253,203,110,0.4); transform: translateY(-2px); }}
    .btn-secondary {{ background: linear-gradient(135deg, #FFEAA7 0%, #FDCB6E 100%); color: #2C3E50; }}
    .btn-secondary:hover {{ box-shadow: 0 6px 16px rgba(255,234,167,0.4); transform: translateY(-2px); }}
    .ai-analysis {{ background: linear-gradient(135deg, #DFE6E9 0%, #B2BEC3 100%); padding: 20px; border-radius: 10px; margin: 20px 0; border-left: 5px solid #74B9FF; }}
    .ai-analysis h3 {{ color: #0984E3; margin-top: 0; font-size: 18px; }}
    .ai-analysis p {{ color: #2C3E50; margin: 8px 0; }}
    .footer {{ text-align: center; padding: 20px; background: linear-gradient(135deg, #F8F9FA 0%, #E9ECEF 100%); color: #7F8C8D; font-size: 13px; border-radius: 0 0 15px 15px; }}
    .button-desc {{ font-size: 12px; margin: 5px 0 15px 0; color: #7F8C8D; }}
    strong {{ color: #2C3E50; }}
</style>
</head>
<body>
<div class="container">
<div class="header">
<h2>‚úÖ Kontakt erkannt und verarbeitet</h2>
</div>
<div class="content">
<div class="info-box">
<h3>üë§ Kontakt:</h3>
<p><strong>{contact_name}</strong></p>
<p>Von: {from_contact}</p>
</div>
<div class="info-box">
<h3>üìù Nachricht:</h3>
<p>{content_preview}</p>
</div>
<div class="ai-analysis">
<h3>ü§ñ Verarbeitung:</h3>
<p>{summary}</p>
<p><strong>Absicht:</strong> {ai_analysis.get('intent', 'unbekannt')}</p>
<p><strong>Dringlichkeit:</strong> {ai_analysis.get('urgency', 'unbekannt')}</p>
{attachments_html}
</div>
<div class="action-buttons">
<h3>üîó Aktionen:</h3>
{buttons_html}
</div>
</div>
<div class="footer">
<p>ü§ñ Automatisch generiert vom C&D Lead Management System</p>
</div>
</div>
</body>
</html>
"""

async def send_final_notification(processing_result: Dict[str, Any], message_type: str, from_contact: str, content: str):
    """
    üéØ FINAL ZAPIER NOTIFICATION - Email an Markus & Info
    
    Wird nach jedem erfolgreichen AI Processing aufgerufen
    ENHANCED: Spezielle Behandlung f√ºr unbekannte Kontakte
    """
    
    ZAPIER_WEBHOOK_URL = "https://hooks.zapier.com/hooks/catch/17762912/u5ilur9/"
    
    contact_match = processing_result.get("contact_match", {})
    contact_found = contact_match.get("found", False)
    
    # üÜï ENHANCED: Unknown Contact Notification (Zapier-compatible format)
    if not contact_found:
        # Check for potential matches from fuzzy search
        potential_matches = processing_result.get("potential_matches", [])
        
        # Build action options based on whether we have potential matches
        action_options = []
        
        if potential_matches:
            # Add "KONTAKT ZUORDNEN" as first option if we found potential matches
            for match in potential_matches[:1]:  # Only show best match in button
                action_options.append({
                    "action": "assign_to_existing",
                    "label": "KONTAKT ZUORDNEN",
                    "description": f"Zu existierendem Kontakt zuordnen: {match.get('contact_name')} ({match.get('reason')})",
                    "contact_id": match.get("contact_id"),
                    "existing_contact": match.get("contact_name")
                })
        
        # Standard actions (always include "add to existing" and "create supplier")
        action_options.extend([
            {
                "action": "create_contact",
                "label": "‚úÖ KONTAKT ANLEGEN",
                "description": "Als neuen Kontakt in WeClapp anlegen"
            },
            {
                "action": "create_supplier",
                "label": "üè≠ LIEFERANT ANLEGEN",
                "description": "Als neuen Lieferanten in WeClapp anlegen"
            },
            {
                "action": "add_to_existing",
                "label": "‚ûï ZU BESTEHENDEM HINZUF√úGEN",
                "description": "Email/Telefon zu einem bestehenden Kontakt hinzuf√ºgen",
                "requires_search": True
            },
            {
                "action": "mark_private",
                "label": "üîí PRIVAT MARKIEREN",
                "description": "Als private Anfrage markieren (kein CRM-Eintrag)"
            },
            {
                "action": "mark_spam",
                "label": "üö´ SPAM MARKIEREN",
                "description": "Als Spam markieren und blockieren"
            },
            {
                "action": "request_info",
                "label": "üì® INFO ANFORDERN",
                "description": "Mehr Informationen vom Absender anfordern"
            },
            {
                "action": "data_good",
                "label": "‚úÖ DATEN OK",
                "description": "Alle Daten korrekt ausgewertet, keine Fehler",
                "color": "success"
            },
            {
                "action": "data_error",
                "label": "‚ö†Ô∏è FEHLER MELDEN",
                "description": "Daten falsch erkannt oder Verarbeitungsfehler",
                "color": "warning"
            },
            {
                "action": "report_issue",
                "label": "üêõ PROBLEM MELDEN",
                "description": "Fehlverhalten oder fehlende Funktion melden",
                "color": "secondary"
            }
        ])
        
        # Build subject and body based on message type
        if message_type == "call":
            call_data = processing_result.get("additional_data", {})
            call_direction = call_data.get("call_direction", "inbound")
            call_duration = call_data.get("call_duration", 0)
            caller_name = call_data.get("caller_name", "")
            phone = from_contact or "Unbekannt"
            
            subject = f"CALL: Anruf {call_direction} - {phone}"
            if caller_name:
                subject += f" ({caller_name})"
            
            sender_display = f"{caller_name} ({phone})" if caller_name else phone
        elif message_type == "email":
            # Get actual email subject from processing_result if available
            email_subject = processing_result.get("ai_analysis", {}).get("email_subject", content[:80])
            attachments_count = processing_result.get("attachments_count", 0)
            
            # Add attachment count to subject if present
            if attachments_count > 0:
                subject = f"EMAIL (üìé {attachments_count}): {email_subject}"
            else:
                subject = f"EMAIL: {email_subject}"
            
            sender_display = from_contact
        else:
            subject = f"{message_type.upper()}: {content[:100]}"
            sender_display = from_contact
        
        notification_data = {
            "notification_type": "unknown_contact_action_required",
            "email_id": f"railway-{now_berlin().timestamp()}",
            "sender": from_contact,
            "sender_name": processing_result.get("sender_name", "Unbekannt"),
            "sender_display": sender_display,
            "subject": subject,
            "body_preview": content[:500] + "..." if len(content) > 500 else content,
            "received_time": now_berlin().isoformat(),
            "message_type": message_type,
            
            # AI Analysis (flattened for Zapier)
            "ai_analysis": processing_result.get("ai_analysis", {}),
            
            # Attachments Info
            "attachments_count": processing_result.get("attachments_count", 0),
            "has_attachments": processing_result.get("has_attachments", False),
            "attachment_results": processing_result.get("attachment_results", []),
            
            # Potential Matches (if any)
            "potential_matches": potential_matches,
            "has_potential_matches": len(potential_matches) > 0,
            
            # Action Options (dynamic based on potential matches)
            "action_options": action_options,
            
            # üí∞ Price Estimate (if available for calls)
            "price_estimate": processing_result.get("price_estimate"),
            "has_price_estimate": processing_result.get("price_estimate", {}).get("found", False) if processing_result.get("price_estimate") else False,
            
            "responsible_employee": "mj@cdtechnologies.de",
            "webhook_reply_url": "https://my-langgraph-agent-production.up.railway.app/webhook/contact-action"
        }
        
        # üé® Generate complete HTML for email
        notification_data["html_body"] = generate_notification_html(notification_data)
        
        # üîç DEBUG: Log attachment info before sending
        logger.info(f"üìé DEBUG notification_data: attachments_count={notification_data.get('attachments_count')}, has_attachments={notification_data.get('has_attachments')}")
        
        # üîç DEBUG: Check if html_body contains attachment line
        html_body = notification_data.get("html_body", "")
        if "Anh√§nge:" in html_body:
            logger.info(f"‚úÖ notification_data['html_body'] CONTAINS 'Anh√§nge:' before Zapier send")
        else:
            logger.warning(f"‚ùå notification_data['html_body'] does NOT contain 'Anh√§nge:' before Zapier send!")
        
        logger.info(f"‚ö†Ô∏è Sending UNKNOWN CONTACT notification for {from_contact}")
    
    else:
        # Standard Notification for known contacts
        # Add feedback/report option even for successful processing
        standard_actions = [
            {
                "action": "view_in_crm",
                "label": "üìã IN CRM √ñFFNEN",
                "description": f"Kontakt in WeClapp √∂ffnen",
                "contact_id": contact_match.get("contact_id"),
                "url": f"https://cundd.weclapp.com/webapp/view/party/{contact_match.get('contact_id')}"
            },
            {
                "action": "data_good",
                "label": "‚úÖ DATEN OK",
                "description": "Alle Daten korrekt ausgewertet und zugeordnet",
                "color": "success"
            },
            {
                "action": "data_error",
                "label": "‚ö†Ô∏è FEHLER MELDEN",
                "description": "Daten falsch erkannt, Zuordnung inkorrekt oder Verarbeitungsfehler",
                "color": "warning"
            },
            {
                "action": "report_issue",
                "label": "üêõ PROBLEM MELDEN",
                "description": "Fehlverhalten, falsche Zuordnung oder fehlende Funktion melden",
                "color": "secondary"
            }
        ]
        
        notification_data = {
            "notification_type": "standard",
            "timestamp": now_berlin().isoformat(),
            "channel": message_type,
            "from": from_contact,
            "content_preview": content[:200] + "..." if len(content) > 200 else content,
            
            # AI Processing Results
            "success": processing_result.get("success", False),
            "workflow_path": processing_result.get("workflow_path"),
            "contact_match": contact_match,
            "ai_analysis": processing_result.get("ai_analysis", {}),
            "tasks_generated": processing_result.get("tasks_generated", []),
            "processing_complete": processing_result.get("processing_complete", False),
            
            # Action buttons (including feedback)
            "action_options": standard_actions,
            
            # Email Recipients
            "recipients": ["mj@cdtechnologies.de", "info@cdtechnologies.de"],
            
            # Notification Details
            "subject": f"ü§ñ C&D AI: {message_type.upper()} von {contact_match.get('contact_name', from_contact)}",
            "summary": f"AI hat {len(processing_result.get('tasks_generated', []))} Tasks erstellt"
        }
        
        # üé® Generate complete HTML for email
        notification_data["html_body"] = generate_notification_html(notification_data)
        
        logger.info(f"‚úÖ Sending standard notification for known contact: {contact_match.get('contact_name', from_contact)}")
    
    try:
        async with aiohttp.ClientSession() as session:
            async with session.post(
                ZAPIER_WEBHOOK_URL,
                json=notification_data,
                timeout=aiohttp.ClientTimeout(total=10)
            ) as response:
                if response.status == 200:
                    logger.info(f"‚úÖ Zapier notification sent successfully for {message_type}")
                    return True
                else:
                    logger.warning(f"‚ö†Ô∏è Zapier notification failed - Status: {response.status}")
                    return False
                    
    except Exception as e:
        logger.error(f"‚ùå Zapier notification error: {e}")
        return False


def _extract_company_from_email(email: str) -> str:
    """Extract potential company name from email domain"""
    if "@" in email:
        domain = email.split("@")[1].split(".")[0]
        return domain.capitalize()
    return ""


def _suggest_contact_type(processing_result: Dict[str, Any]) -> str:
    """Suggest contact type based on AI analysis"""
    intent = processing_result.get("ai_analysis", {}).get("intent", "")
    
    if intent in ["quote_request", "appointment", "project_inquiry"]:
        return "customer"
    elif intent in ["invoice", "delivery", "order"]:
        return "supplier"
    else:
        return "prospect"

# ===============================
# SQLITE CONTACT CACHE LAYER
# ===============================

DB_PATH = os.getenv("DATABASE_PATH", "./email_data.db")

def initialize_contact_cache():
    """
    üóÑÔ∏è SQLITE EMAIL DATABASE - Performance Layer
    
    Nutzt die existierende email_data.db die von weclapp-sql-sync-production Actor gef√ºllt wird.
    MASTER PLAN: Erste Anlaufstelle vor WeClapp API Call!
    """
    conn = sqlite3.connect(DB_PATH)
    cursor = conn.cursor()
    
    cursor.execute("""
    CREATE TABLE IF NOT EXISTS email_data (
        id INTEGER PRIMARY KEY AUTOINCREMENT,
        subject TEXT,
        sender TEXT,
        recipient TEXT,
        received_date TEXT,
        ocr_text TEXT,
        gpt_result TEXT,
        weclapp_contact_id TEXT,
        weclapp_customer_id TEXT,
        weclapp_opportunity_id TEXT,
        current_stage TEXT,
        gpt_status_suggestion TEXT,
        status_deviation BOOLEAN,
        remarks TEXT
    )
    """)
    
    # üîß MIGRATION: Add new columns if they don't exist (for old databases)
    try:
        # Check if message_type column exists
        cursor.execute("PRAGMA table_info(email_data)")
        columns = [col[1] for col in cursor.fetchall()]
        
        if "message_type" not in columns:
            logger.info("üîß MIGRATION: Adding new columns to email_data table...")
            cursor.execute("ALTER TABLE email_data ADD COLUMN message_type TEXT")
            cursor.execute("ALTER TABLE email_data ADD COLUMN direction TEXT")
            cursor.execute("ALTER TABLE email_data ADD COLUMN workflow_path TEXT")
            cursor.execute("ALTER TABLE email_data ADD COLUMN ai_intent TEXT")
            cursor.execute("ALTER TABLE email_data ADD COLUMN ai_urgency TEXT")
            cursor.execute("ALTER TABLE email_data ADD COLUMN ai_sentiment TEXT")
            cursor.execute("ALTER TABLE email_data ADD COLUMN attachments_count INTEGER DEFAULT 0")
            cursor.execute("ALTER TABLE email_data ADD COLUMN processing_timestamp TEXT")
            cursor.execute("ALTER TABLE email_data ADD COLUMN processing_duration_ms INTEGER")
            cursor.execute("ALTER TABLE email_data ADD COLUMN price_estimate_json TEXT")
            conn.commit()
            logger.info("‚úÖ MIGRATION: New columns added successfully")
    except Exception as e:
        logger.warning(f"‚ö†Ô∏è Migration warning (likely already applied): {e}")
    
    # Index f√ºr schnelle Email-Lookups
    cursor.execute("""
    CREATE INDEX IF NOT EXISTS idx_sender ON email_data(sender)
    """)
    
    # Create email_attachments table (for attachment tracking)
    cursor.execute("""
    CREATE TABLE IF NOT EXISTS email_attachments (
        id INTEGER PRIMARY KEY AUTOINCREMENT,
        email_message_id TEXT NOT NULL,
        filename TEXT NOT NULL,
        content_type TEXT,
        size_bytes INTEGER,
        file_hash TEXT,
        ocr_text TEXT,
        ocr_route TEXT,
        onedrive_path TEXT,
        onedrive_link TEXT,
        processed_date TEXT NOT NULL
    )
    """)
    
    cursor.execute("""
    CREATE INDEX IF NOT EXISTS idx_email_message_id ON email_attachments(email_message_id)
    """)
    
    conn.commit()
    conn.close()
    logger.info("‚úÖ Email Database initialized (email_data.db + email_attachments)")


async def lookup_contact_in_cache(email: str) -> Optional[Dict[str, Any]]:
    """
    üîç STEP 1: Multi-Source Contact Lookup (Cache ‚Üí WEClapp Sync DB ‚Üí WeClapp API)
    
    Sucht Kontakt in mehreren Quellen:
    1. Lokaler email_data.db Cache (schnellster)
    2. WEClapp Sync Database (von Apify Actor)
    3. WeClapp API (langsamster, nur als Fallback)
    """
    try:
        # STEP 1a: Check local email_data cache first
        loop = asyncio.get_event_loop()
        result = await loop.run_in_executor(None, _sync_lookup_contact, email)
        
        if result:
            logger.info(f"‚úÖ CACHE HIT (email_data) for {email} - Contact ID: {result['weclapp_contact_id']}")
            return result
        
        # STEP 1b: Check WEClapp Sync DB (from Apify actor)
        logger.info(f"üîç Checking WEClapp Sync DB for {email}...")
        
        # Ensure WEClapp DB is available
        await ensure_weclapp_db_available()
        
        weclapp_contact = await query_weclapp_contact(email)
        
        if weclapp_contact:
            logger.info(f"‚úÖ WECLAPP SYNC DB HIT for {email} - {weclapp_contact.get('name', 'Unknown')}")
            
            # Convert to expected format
            result = {
                "found": True,
                "source": "weclapp_sync_db",
                "weclapp_contact_id": weclapp_contact.get("party_id") or weclapp_contact.get("lead_id"),
                "weclapp_customer_id": weclapp_contact.get("customer_number"),
                "sender": email,
                "name": weclapp_contact.get("name"),
                "phone": weclapp_contact.get("phone"),
                "company": weclapp_contact.get("company"),
                "party_type": weclapp_contact.get("party_type") or "LEAD"
            }
            
            return result
        
        logger.info(f"‚ö†Ô∏è CACHE MISS (all sources) for {email} - Will query WeClapp API")
        return None
            
    except Exception as e:
        logger.error(f"‚ùå Cache lookup error: {e}")
        return None


def _sync_lookup_contact(email: str) -> Optional[Dict[str, Any]]:
    """Synchronous SQLite lookup in email_data.db (runs in executor)"""
    conn = sqlite3.connect(DB_PATH)
    cursor = conn.cursor()
    
    cursor.execute("""
    SELECT DISTINCT weclapp_contact_id, weclapp_customer_id, sender
    FROM email_data
    WHERE sender = ? AND weclapp_contact_id IS NOT NULL
    ORDER BY id DESC
    LIMIT 1
    """, (email.lower(),))
    
    row = cursor.fetchone()
    conn.close()
    
    if row:
        result = {
            "found": True,
            "source": "cache",
            "weclapp_contact_id": row[0],
            "weclapp_customer_id": row[1],
            "sender": row[2]
        }
        conn.close()
        return result
    
    conn.close()
    return None


async def cache_contact(email: str, weclapp_data: Dict[str, Any]):
    """
    üíæ STEP 2: Cache Write-Back nach WeClapp Lookup
    
    Speichert WeClapp Contact in lokalem Cache f√ºr zuk√ºnftige Lookups.
    """
    try:
        loop = asyncio.get_event_loop()
        await loop.run_in_executor(None, _sync_cache_contact, email, weclapp_data)
        logger.info(f"‚úÖ Contact cached: {email} ‚Üí {weclapp_data.get('weclapp_contact_id')}")
    except Exception as e:
        logger.error(f"‚ùå Cache write error: {e}")


def _sync_cache_contact(email: str, weclapp_data: Dict[str, Any]):
    """Synchronous SQLite write (runs in executor)"""
    conn = sqlite3.connect(DB_PATH)
    cursor = conn.cursor()
    
    cursor.execute("""
    INSERT INTO email_data (sender, weclapp_contact_id, weclapp_customer_id, received_date, subject)
    VALUES (?, ?, ?, ?, ?)
    """, (
        email.lower(),
        weclapp_data.get("weclapp_contact_id"),
        weclapp_data.get("weclapp_customer_id"),
        now_berlin().isoformat(),
        "Contact cached from WeClapp lookup"
    ))
    
    conn.commit()
    conn.close()
    logger.info(f"‚úÖ Cached in email_data.db: {email} ‚Üí {weclapp_data.get('weclapp_contact_id')}")


# ===============================
# DUPLIKATSPR√úFUNG & DOKUMENT-TRACKING
# ===============================

async def check_document_duplicate(
    message_id: str = None,
    sender: str = None,
    subject: str = None,
    attachment_filename: str = None,
    document_hash: str = None,
    onedrive_path: str = None
) -> Dict[str, Any]:
    """
    üîç Pr√ºft ob Dokument bereits verarbeitet wurde
    
    Pr√ºfkriterien (in Reihenfolge):
    1. message_id (Microsoft Graph ID) - 100% sicher
    2. document_hash (SHA256 des PDF) - sehr sicher
    3. onedrive_path - wenn bereits hochgeladen
    4. sender + subject + filename + zeitfenster (24h) - heuristisch
    
    Returns:
        {
            "is_duplicate": bool,
            "duplicate_reason": str,
            "original_record_id": int,
            "original_timestamp": str,
            "onedrive_link": str
        }
    """
    try:
        loop = asyncio.get_event_loop()
        result = await loop.run_in_executor(
            None, 
            _check_duplicate_sync,
            message_id, sender, subject, attachment_filename, document_hash, onedrive_path
        )
        
        if result["is_duplicate"]:
            logger.warning(f"‚ö†Ô∏è DUPLICATE: {result['duplicate_reason']}")
        else:
            logger.info(f"‚úÖ No duplicate found - proceeding with processing")
        
        return result
        
    except Exception as e:
        logger.error(f"‚ùå Duplicate check error: {e}")
        # Bei Fehler: Kein Duplikat annehmen (lieber verarbeiten)
        return {"is_duplicate": False, "error": str(e)}


def _check_duplicate_sync(
    message_id: str,
    sender: str,
    subject: str,
    attachment_filename: str,
    document_hash: str,
    onedrive_path: str
) -> Dict[str, Any]:
    """Synchronous duplicate check (runs in executor)"""
    conn = sqlite3.connect(DB_PATH)
    cursor = conn.cursor()
    
    try:
        # PR√úFUNG 1: message_id (100% sicher - Microsoft Graph ID)
        if message_id:
            cursor.execute("""
            SELECT id, processing_timestamp, onedrive_link
            FROM processed_documents
            WHERE message_id = ?
            LIMIT 1
            """, (message_id,))
            
            row = cursor.fetchone()
            if row:
                return {
                    "is_duplicate": True,
                    "duplicate_reason": "message_id_match",
                    "original_record_id": row[0],
                    "original_timestamp": row[1],
                    "onedrive_link": row[2]
                }
        
        # PR√úFUNG 2: document_hash (sehr sicher - SHA256 des PDFs)
        if document_hash:
            cursor.execute("""
            SELECT id, processing_timestamp, onedrive_link
            FROM processed_documents
            WHERE document_hash = ?
            LIMIT 1
            """, (document_hash,))
            
            row = cursor.fetchone()
            if row:
                return {
                    "is_duplicate": True,
                    "duplicate_reason": "document_hash_match",
                    "original_record_id": row[0],
                    "original_timestamp": row[1],
                    "onedrive_link": row[2]
                }
        
        # PR√úFUNG 3: onedrive_path (wenn bereits hochgeladen)
        if onedrive_path:
            cursor.execute("""
            SELECT id, processing_timestamp, onedrive_link
            FROM processed_documents
            WHERE onedrive_path = ?
            LIMIT 1
            """, (onedrive_path,))
            
            row = cursor.fetchone()
            if row:
                return {
                    "is_duplicate": True,
                    "duplicate_reason": "onedrive_path_match",
                    "original_record_id": row[0],
                    "original_timestamp": row[1],
                    "onedrive_link": row[2]
                }
        
        # PR√úFUNG 4: Heuristisch (sender + subject + filename + 24h)
        if sender and subject and attachment_filename:
            # Zeitfenster: 24 Stunden
            time_24h_ago = (datetime.now(BERLIN_TZ) - timedelta(hours=24)).isoformat()
            
            cursor.execute("""
            SELECT id, processing_timestamp, onedrive_link
            FROM processed_documents
            WHERE sender = ?
              AND subject = ?
              AND attachment_filename = ?
              AND processing_timestamp > ?
            LIMIT 1
            """, (sender, subject, attachment_filename, time_24h_ago))
            
            row = cursor.fetchone()
            if row:
                return {
                    "is_duplicate": True,
                    "duplicate_reason": "heuristic_match_24h",
                    "original_record_id": row[0],
                    "original_timestamp": row[1],
                    "onedrive_link": row[2]
                }
        
        # KEIN DUPLIKAT GEFUNDEN
        return {"is_duplicate": False}
        
    finally:
        conn.close()


def _save_processed_document_sync(
    message_id: str,
    sender: str,
    subject: str,
    attachment_filename: str,
    document_hash: str,
    document_type: str,
    onedrive_path: str,
    onedrive_link: str,
    ordnerstruktur: str,
    kunde: str,
    lieferant: str,
    summe: str
) -> int:
    """
    üíæ Speichert verarbeitetes Dokument in processed_documents Tabelle
    
    Returns: record_id
    """
    conn = sqlite3.connect(DB_PATH)
    cursor = conn.cursor()
    
    try:
        # Tabelle erstellen falls nicht vorhanden
        cursor.execute("""
        CREATE TABLE IF NOT EXISTS processed_documents (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            message_id TEXT UNIQUE,
            sender TEXT,
            subject TEXT,
            attachment_filename TEXT,
            document_hash TEXT UNIQUE,
            document_type TEXT,
            onedrive_path TEXT UNIQUE,
            onedrive_link TEXT,
            ordnerstruktur TEXT,
            kunde TEXT,
            lieferant TEXT,
            summe TEXT,
            processing_timestamp TEXT,
            created_at TEXT DEFAULT CURRENT_TIMESTAMP
        )
        """)
        
        # CREATE INDEX wenn nicht vorhanden
        cursor.execute("""
        CREATE INDEX IF NOT EXISTS idx_message_id ON processed_documents(message_id)
        """)
        cursor.execute("""
        CREATE INDEX IF NOT EXISTS idx_document_hash ON processed_documents(document_hash)
        """)
        cursor.execute("""
        CREATE INDEX IF NOT EXISTS idx_onedrive_path ON processed_documents(onedrive_path)
        """)
        cursor.execute("""
        CREATE INDEX IF NOT EXISTS idx_sender_subject_filename ON processed_documents(sender, subject, attachment_filename)
        """)
        
        # INSERT
        cursor.execute("""
        INSERT INTO processed_documents (
            message_id, sender, subject, attachment_filename, document_hash,
            document_type, onedrive_path, onedrive_link, ordnerstruktur,
            kunde, lieferant, summe, processing_timestamp
        ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
        """, (
            message_id, sender, subject, attachment_filename, document_hash,
            document_type, onedrive_path, onedrive_link, ordnerstruktur,
            kunde, lieferant, summe, now_berlin().isoformat()
        ))
        
        record_id = cursor.lastrowid
        conn.commit()
        
        logger.info(f"‚úÖ Saved processed document: ID={record_id}, Type={document_type}, Path={onedrive_path}")
        return record_id
        
    except sqlite3.IntegrityError as e:
        # UNIQUE constraint violation - ist ein Duplikat!
        logger.warning(f"‚ö†Ô∏è Document already exists in DB: {e}")
        conn.rollback()
        
        # Hole existing record
        if message_id:
            cursor.execute("SELECT id FROM processed_documents WHERE message_id = ?", (message_id,))
        elif document_hash:
            cursor.execute("SELECT id FROM processed_documents WHERE document_hash = ?", (document_hash,))
        else:
            cursor.execute("SELECT id FROM processed_documents WHERE onedrive_path = ?", (onedrive_path,))
        
        row = cursor.fetchone()
        return row[0] if row else None
        
    except Exception as e:
        logger.error(f"‚ùå Error saving processed document: {e}")
        conn.rollback()
        raise
    finally:
        conn.close()

# ===============================
# LANGRAPH STATE DEFINITIONS
# ===============================

class CommunicationState(TypedDict):
    """LangGraph State f√ºr AI Communication Processing"""
    
    # Input Data
    message_type: str  # "email", "call", "whatsapp"
    from_contact: str
    content: str
    timestamp: str
    additional_data: Optional[Dict[str, Any]]  # Email attachments, call metadata, etc.
    
    # Processing Results
    contact_match: Optional[Dict[str, Any]]
    ai_analysis: Optional[Dict[str, Any]]
    workflow_path: Optional[str]  # "WEG_A" or "WEG_B"
    tasks_generated: List[Dict[str, Any]]
    crm_updates: List[Dict[str, Any]]
    price_estimate: Optional[Dict[str, Any]]  # üí∞ NEW: Automatic price estimation from calls
    
    # Final Output
    processing_complete: bool
    response_sent: bool
    errors: List[str]

@dataclass
class ContactMatch:
    """Contact Matching Result"""
    found: bool
    contact_id: Optional[str] = None
    contact_name: Optional[str] = None
    company: Optional[str] = None
    confidence: float = 0.0
    source: Optional[str] = None  # "cache", "weclapp", "apify", "manual"
    cache_hits: int = 0  # How many times this contact was found in cache

@dataclass
class AITask:
    """Generated AI Task"""
    title: str
    description: str
    assigned_to: str
    priority: str  # "low", "medium", "high", "urgent"
    due_date: str
    task_type: str  # "follow_up", "quote", "support", "meeting"
    contact_id: Optional[str] = None

# ==================== WECLAPP SYNC DATABASE INTEGRATION ====================

async def download_weclapp_db_from_onedrive(access_token_onedrive: str) -> Optional[str]:
    """
    Downloads WEClapp sync database from OneDrive
    Tries multiple possible locations automatically
    
    Returns: Local path to DB file or None if failed
    """
    logger.info("üì• Downloading WEClapp Sync DB from OneDrive...")
    
    user_email = "mj@cdtechnologies.de"
    local_path = "/tmp/weclapp_sync.db"
    
    # Try multiple possible OneDrive locations
    possible_paths = [
        "/Temp/weclapp_sync.db",
        "/Temp/weclapp_data.db",
        "/scan/weclapp_sync.db",
        "/scan/weclapp_data.db",
        "/Email/weclapp_sync.db",
        "/Database/weclapp_sync.db",
        "/weclapp_sync.db",
        "/weclapp_data.db"
    ]
    
    for try_path in possible_paths:
        logger.info(f"üîç Trying OneDrive path: {try_path}")
        
        download_url = f"https://graph.microsoft.com/v1.0/users/{user_email}/drive/root:{try_path}:/content"
        
        headers = {"Authorization": f"Bearer {access_token_onedrive}"}
        
        try:
            async with httpx.AsyncClient(timeout=60.0) as client:
                response = await client.get(download_url, headers=headers)
                
                if response.status_code == 200:
                    file_bytes = response.content
                    file_size_kb = len(file_bytes) / 1024
                    
                    logger.info(f"‚úÖ Found WEClapp DB at {try_path} ({file_size_kb:.1f} KB)")
                    
                    # Save locally
                    with open(local_path, 'wb') as f:
                        f.write(file_bytes)
                    
                    logger.info(f"üíæ Saved WEClapp DB to {local_path}")
                    return local_path
                
                elif response.status_code == 404:
                    logger.debug(f"‚ö†Ô∏è Not found at {try_path}")
                    continue
                else:
                    logger.warning(f"‚ö†Ô∏è Error {response.status_code} at {try_path}")
                    continue
                    
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Download error for {try_path}: {str(e)}")
            continue
    
    logger.warning("‚ùå Could not find WEClapp DB at any expected OneDrive location")
    return None


async def query_weclapp_contact(sender_email: str) -> Optional[Dict]:
    """
    Query WEClapp sync database for contact information
    
    Returns: Contact info dict or None if not found
    """
    db_path = "/tmp/weclapp_sync.db"
    
    if not os.path.exists(db_path):
        logger.debug(f"‚ö†Ô∏è WEClapp DB not found at {db_path} - download needed")
        return None
    
    try:
        import sqlite3
        conn = sqlite3.connect(db_path)
        cursor = conn.cursor()
        
        # Try parties table first (customers/suppliers)
        cursor.execute("""
            SELECT id, name, email, phone, customerNumber, partyType
            FROM parties 
            WHERE email = ? COLLATE NOCASE
            LIMIT 1
        """, (sender_email,))
        
        row = cursor.fetchone()
        
        if row:
            conn.close()
            logger.info(f"‚úÖ Found contact in WEClapp Sync DB: {row[1]} (Party ID: {row[0]})")
            
            return {
                "party_id": row[0],
                "name": row[1],
                "email": row[2],
                "phone": row[3],
                "customer_number": row[4],
                "party_type": row[5],
                "source": "weclapp_sync_db"
            }
        
        # Try leads table
        cursor.execute("""
            SELECT id, firstName, lastName, email, phone, company, leadStatus
            FROM leads
            WHERE email = ? COLLATE NOCASE
            LIMIT 1
        """, (sender_email,))
        
        row = cursor.fetchone()
        
        if row:
            conn.close()
            name = f"{row[1]} {row[2]}".strip()
            logger.info(f"‚úÖ Found lead in WEClapp Sync DB: {name} (Lead ID: {row[0]})")
            
            return {
                "lead_id": row[0],
                "name": name,
                "email": row[3],
                "phone": row[4],
                "company": row[5],
                "lead_status": row[6],
                "source": "weclapp_sync_db"
            }
        
        conn.close()
        logger.debug(f"‚ö†Ô∏è No contact found for {sender_email} in WEClapp Sync DB")
        return None
        
    except Exception as e:
        logger.error(f"‚ùå WEClapp DB query error: {str(e)}")
        return None


# Global flag to track if WEClapp DB was downloaded this session
WECLAPP_DB_DOWNLOADED = False

async def ensure_weclapp_db_available() -> bool:
    """
    Ensures WEClapp sync database is available locally
    Downloads from OneDrive if needed (once per session)
    
    Returns: True if DB is available, False otherwise
    """
    global WECLAPP_DB_DOWNLOADED
    
    db_path = "/tmp/weclapp_sync.db"
    
    # Check if already downloaded this session
    if WECLAPP_DB_DOWNLOADED and os.path.exists(db_path):
        logger.debug("‚úÖ WEClapp DB already available (cached)")
        return True
    
    # Check if file exists from previous session
    if os.path.exists(db_path):
        # Check age - if older than 1 hour, re-download
        file_age_seconds = datetime.now().timestamp() - os.path.getmtime(db_path)
        if file_age_seconds < 3600:  # 1 hour
            logger.info(f"‚úÖ WEClapp DB available (age: {file_age_seconds/60:.1f} min)")
            WECLAPP_DB_DOWNLOADED = True
            return True
        else:
            logger.info(f"üîÑ WEClapp DB outdated (age: {file_age_seconds/3600:.1f} hours), re-downloading...")
    
    # Need to download
    try:
        access_token = await get_graph_token_onedrive()
        if not access_token:
            logger.warning("‚ö†Ô∏è Could not get OneDrive token for WEClapp DB download")
            return False
        
        downloaded_path = await download_weclapp_db_from_onedrive(access_token)
        
        if downloaded_path:
            WECLAPP_DB_DOWNLOADED = True
            logger.info("‚úÖ WEClapp Sync DB successfully downloaded and ready")
            return True
        else:
            logger.warning("‚ö†Ô∏è WEClapp Sync DB download failed")
            return False
            
    except Exception as e:
        logger.error(f"‚ùå WEClapp DB download error: {str(e)}")
        return False


async def get_graph_token_onedrive():
    """Holt das Zugriffstoken von Microsoft Graph f√ºr OneDrive."""
    tenant_id = os.getenv("GRAPH_TENANT_ID_ONEDRIVE") or os.getenv("GRAPH_TENANT_ID")
    client_id = os.getenv("GRAPH_CLIENT_ID_ONEDRIVE") or os.getenv("GRAPH_CLIENT_ID")
    client_secret = os.getenv("GRAPH_CLIENT_SECRET_ONEDRIVE") or os.getenv("GRAPH_CLIENT_SECRET")
    
    if not tenant_id or not client_id or not client_secret:
        logger.error("‚ùå Fehlende OneDrive-Graph API Zugangsdaten")
        return None
    
    url = f"https://login.microsoftonline.com/{tenant_id}/oauth2/v2.0/token"
    headers = {"Content-Type": "application/x-www-form-urlencoded"}
    data = {
        "grant_type": "client_credentials",
        "client_id": client_id,
        "client_secret": client_secret,
        "scope": "https://graph.microsoft.com/.default"
    }
    
    try:
        async with httpx.AsyncClient() as client:
            response = await client.post(url, headers=headers, data=data)
            if response.status_code == 200:
                return response.json().get("access_token")
            else:
                logger.error(f"‚ùå OneDrive token error {response.status_code}: {response.text}")
                return None
    except Exception as e:
        logger.error(f"‚ùå OneDrive token exception: {e}")
        return None


# ==================== END WECLAPP SYNC INTEGRATION ====================

async def save_to_database(
    processing_result: Dict,
    message_type: str,
    from_contact: str,
    content: str,
    final_state: Dict,
    message_id: str = ""
) -> int:
    """üíæ Save processing results to database and return record ID"""
    import asyncio
    
    try:
        loop = asyncio.get_event_loop()
        record_id = await loop.run_in_executor(None, _save_to_db_sync, processing_result, message_type, from_contact, content, final_state, message_id)
        return record_id
    except Exception as e:
        logger.error(f"‚ùå DB save error: {e}")
        raise


def _save_to_db_sync(processing_result, message_type, from_contact, content, final_state, message_id="") -> int:
    """Synchronous database operations - returns record ID"""
    import sqlite3
    conn = sqlite3.connect(DB_PATH)
    cursor = conn.cursor()
    
    try:
        ai_analysis = processing_result.get("ai_analysis", {})
        additional_data = final_state.get("additional_data", {})
        
        cursor.execute("""
        INSERT INTO email_data (
            subject, sender, recipient, received_date, gpt_result,
            message_type, direction, workflow_path,
            ai_intent, ai_urgency, ai_sentiment,
            attachments_count, processing_timestamp
        ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
        """, (
            ai_analysis.get("email_subject", content[:200]),
            from_contact,
            "mj@cdtechnologies.de",
            now_berlin().isoformat(),
            json.dumps(ai_analysis, ensure_ascii=False),
            message_type,
            additional_data.get("direction", "incoming"),
            processing_result.get("workflow_path"),
            ai_analysis.get("intent"),
            ai_analysis.get("urgency"),
            ai_analysis.get("sentiment"),
            processing_result.get("attachments_count", 0),
            now_berlin().isoformat()
        ))
        
        email_id = cursor.lastrowid
        
        # Insert attachments (use correct table name: email_attachments)
        for att in processing_result.get("attachment_results", []):
            cursor.execute("""
            INSERT INTO email_attachments (
                email_message_id, filename, content_type, size_bytes,
                ocr_text, ocr_route, file_hash, processed_date
            ) VALUES (?, ?, ?, ?, ?, ?, ?, ?)
            """, (
                message_id,
                att.get("filename"),
                att.get("type"),
                att.get("size"),
                att.get("ocr_text", "")[:1000],
                att.get("ocr_route", ""),
                att.get("file_hash", ""),
                now_berlin().isoformat()
            ))
        
        conn.commit()
        logger.info(f"‚úÖ Saved email {email_id} with {len(processing_result.get('attachment_results', []))} attachments")
        return email_id
    except Exception as e:
        conn.rollback()
        raise
    finally:
        conn.close()


# ===============================
# PRODUCTION AI ORCHESTRATOR
# ===============================

class ProductionAIOrchestrator:
    """Production-ready LangGraph AI Communication Orchestrator"""
    
    def __init__(self):
        # Environment Setup with debugging
        self.openai_api_key = os.getenv("OPENAI_API_KEY")
        self.apify_token = os.getenv("APIFY_TOKEN")
        self.weclapp_api_token = os.getenv("WECLAPP_API_TOKEN")
        self.weclapp_domain = os.getenv("WECLAPP_DOMAIN", "cdtech")
        
        # ‚ö†Ô∏è SECURITY: Only log existence, NEVER log actual keys or full env list
        print(f"üîç Environment Check: OpenAI API Key configured: {bool(self.openai_api_key)}")
        print(f"üîç Environment Check: WeClapp API Token configured: {bool(self.weclapp_api_token)}")
        print(f"üîç Environment Check: Apify Token configured: {bool(self.apify_token)}")
        
        if not self.openai_api_key:
            raise ValueError("OPENAI_API_KEY environment variable required!")
        
        # LangChain/OpenAI Setup
        self.llm = ChatOpenAI(
            api_key=self.openai_api_key,
            model="gpt-4-turbo-preview",
            temperature=0.1,
            max_tokens=2000
        )
        
        # JSON Output Parser
        self.json_parser = JsonOutputParser()
        
        # Build LangGraph Workflow
        self.workflow = self._build_langgraph_workflow()
        
        logger.info("‚úÖ Production AI Orchestrator initialized with LangGraph")
    
    def _map_dokumenttyp_to_intent(self, dokumenttyp: str) -> str:
        """Map Apify dokumenttyp to Railway intent"""
        mapping = {
            "rechnung": "sales",
            "eingangsrechnung": "sales",
            "ausgangsrechnung": "sales",
            "angebot": "sales",
            "auftragsbest√§tigung": "sales",
            "lieferschein": "sales",
            "aufma√ü": "sales",
            "anfrage": "information",
            "reklamation": "complaint",
            "beh√∂rdlich": "information",
            "sonstiges": "information"
        }
        return mapping.get(dokumenttyp.lower(), "information")
    
    def _build_langgraph_workflow(self) -> StateGraph:
        """Builds the main LangGraph workflow"""
        
        # Create StateGraph
        workflow = StateGraph(CommunicationState)
        
        # Define workflow nodes
        workflow.add_node("contact_lookup", self._contact_lookup_node)
        workflow.add_node("ai_analysis", self._ai_analysis_node)
        workflow.add_node("workflow_routing", self._workflow_routing_node)
        workflow.add_node("weg_a_unknown_contact", self._weg_a_unknown_contact_node)
        workflow.add_node("weg_b_known_contact", self._weg_b_known_contact_node)
        workflow.add_node("finalize_processing", self._finalize_processing_node)
        
        # Define workflow edges
        workflow.set_entry_point("contact_lookup")
        
        # Contact Lookup ‚Üí AI Analysis
        workflow.add_edge("contact_lookup", "ai_analysis")
        
        # AI Analysis ‚Üí Workflow Routing
        workflow.add_edge("ai_analysis", "workflow_routing")
        
        # Conditional routing based on contact match
        workflow.add_conditional_edges(
            "workflow_routing",
            self._route_workflow_condition,
            {
                "WEG_A": "weg_a_unknown_contact",
                "WEG_B": "weg_b_known_contact"
            }
        )
        
        # Both paths lead to finalization
        workflow.add_edge("weg_a_unknown_contact", "finalize_processing")
        workflow.add_edge("weg_b_known_contact", "finalize_processing")
        
        # Finalization ends the workflow
        workflow.add_edge("finalize_processing", END)
        
        return workflow.compile()
    
    async def _contact_lookup_node(self, state: CommunicationState) -> CommunicationState:
        """LangGraph Node: Contact Lookup in CRM/Database"""
        
        logger.info(f"üîç Contact lookup for: {state['from_contact']}")
        
        try:
            # STEP 1: Direct WeClapp Contact Search (Email or Phone)
            weclapp_match = await self._search_weclapp_contact(state["from_contact"])
            
            # STEP 2: If no direct match, try FUZZY MATCHING
            potential_matches = []
            if not weclapp_match.found:
                logger.info("üîç No direct match - trying fuzzy search...")
                potential_matches = await self._fuzzy_contact_search(state["from_contact"], state)
                
                if potential_matches:
                    logger.info(f"‚ú® Found {len(potential_matches)} potential matches via fuzzy search")
                    # Store potential matches for WEG_A notification
                    state["potential_matches"] = potential_matches
            
            # STEP 3: Apify Contact Search (wenn WeClapp nichts findet)
            if not weclapp_match.found and not potential_matches:
                apify_match = await self._search_apify_contact(state["from_contact"])
                contact_match = apify_match
            else:
                contact_match = weclapp_match
            
            # Ensure contact_match is valid
            if contact_match is None:
                raise ValueError("Contact search returned None")
            
            # Update state with safe attribute access
            state["contact_match"] = {
                "found": getattr(contact_match, "found", False),
                "contact_id": getattr(contact_match, "contact_id", None),
                "contact_name": getattr(contact_match, "contact_name", None),
                "company": getattr(contact_match, "company", None),
                "confidence": getattr(contact_match, "confidence", 0.0),
                "source": getattr(contact_match, "source", "unknown")
            }
            
            logger.info(f"‚úÖ Contact match result: {contact_match.found} ({contact_match.source})")
            
        except Exception as e:
            logger.error(f"‚ùå Contact lookup error: {e}")
            state["errors"].append(f"Contact lookup failed: {e}")
            state["contact_match"] = {
                "found": False,
                "contact_id": None,
                "contact_name": None,
                "company": None,
                "confidence": 0.0,
                "source": "error"
            }
        
        return state
    
    async def _classify_document_async(
        self, 
        ocr_text: str, 
        handwriting_text: str = "", 
        metadata: dict = None
    ) -> dict:
        """
        ü§ñ ASYNC WRAPPER f√ºr classify_document_with_gpt()
        
        Verwendet die modulare classify_document_with_gpt() Funktion,
        aber macht sie async-kompatibel f√ºr den LangGraph Orchestrator.
        
        Args:
            ocr_text: Extrahierter OCR-Text
            handwriting_text: Optional handschriftlicher Text
            metadata: Zus√§tzliche Metadaten (subject, from, etc.)
        
        Returns:
            GPT-Analyse-Ergebnis (Apify JSON Format)
        """
        import asyncio
        
        logger.info("ü§ñ Using modular classify_document_with_gpt()...")
        
        # Run sync function in executor (thread pool) to make it async
        loop = asyncio.get_event_loop()
        result = await loop.run_in_executor(
            None,  # Use default executor
            classify_document_with_gpt,
            ocr_text,
            handwriting_text,
            metadata or {}
        )
        
        logger.info(f"‚úÖ classify_document_with_gpt() completed: {result.get('dokumenttyp', 'N/A')}")
        return result
    
    async def _ai_analysis_node(self, state: CommunicationState) -> CommunicationState:
        """LangGraph Node: AI Analysis of Communication"""
        
        logger.info(f"ü§ñ AI analyzing {state['message_type']} from {state['from_contact']}")
        
        try:
            # Get additional context
            additional_data = state.get("additional_data", {})
            email_direction = additional_data.get("email_direction", "incoming")
            subject = additional_data.get("subject", "")
            body = additional_data.get("body", "")
            attachments = additional_data.get("attachments", [])
            attachment_names = [att.get("name", "") for att in attachments] if attachments else []
            attachment_results = additional_data.get("attachment_results", [])
            document_type_hint = additional_data.get("document_type_hint")
            
            # üéØ USE APIFY PROMPTS - Much better document classification!
            if attachment_results and len(attachment_results) > 0:
                # ‚úÖ EMAIL WITH PDF ATTACHMENTS ‚Üí analyse_scan_prompt (200 lines!)
                logger.info(f"üìÑ Using modular classify_document_with_gpt() for {len(attachment_results)} attachment(s)")
                
                # Combine OCR text from all attachments
                ocr_text = "\n\n=== N√ÑCHSTES DOKUMENT ===\n\n".join([
                    att.get("ocr_text", "") for att in attachment_results if att.get("ocr_text")
                ])
                
                # Combine handwriting text (if any)
                handwriting_text = "\n\n".join([
                    att.get("handwriting_text", "") for att in attachment_results if att.get("handwriting_text")
                ])
                
                # Metadata for classification
                metadata = {
                    "subject": subject,
                    "from": state['from_contact'],
                    "email_direction": email_direction,
                    "attachments_count": len(attachment_results),
                    "document_type_hint": document_type_hint,
                    "attachment_filenames": [att.get("filename", "") for att in attachment_results]
                }
                
                # ü§ñ USE MODULAR FUNCTION (instead of inline LLM call)
                apify_result = await self._classify_document_async(
                    ocr_text=ocr_text,
                    handwriting_text=handwriting_text,
                    metadata=metadata
                )
                
            else:
                # ‚úÖ EMAIL WITHOUT ATTACHMENTS ‚Üí analyse_mail_prompt
                logger.info(f"üìß Using modular classify_document_with_gpt() for text-only email")
                
                # For text-only emails, we still use classify_document_with_gpt
                # but pass email body as "ocr_text"
                metadata = {
                    "subject": subject,
                    "from": state['from_contact'],
                    "email_direction": email_direction,
                    "document_type_hint": document_type_hint,
                    "is_text_only_email": True
                }
                
                # ü§ñ USE MODULAR FUNCTION
                apify_result = await self._classify_document_async(
                    ocr_text=body or state['content'],
                    handwriting_text="",
                    metadata=metadata
                )
            
            # apify_result already contains parsed JSON from classify_document_with_gpt()
            # No need for manual parsing - the modular function handles it!
            
            # üîÑ MAP APIFY FIELDS ‚Üí RAILWAY FORMAT
            # Apify prompts return: dokumenttyp, richtung, rolle, kunde, lieferant, projektnummer, ordnerstruktur, etc.
            # Railway expects: intent, urgency, sentiment, document_type, suggested_tasks, etc.
            
            ai_result = {
                # Core fields from Apify
                "dokumenttyp": apify_result.get("dokumenttyp", "general"),
                "richtung": apify_result.get("richtung", "eingang"),
                "rolle": apify_result.get("rolle", "kunde"),
                "kunde": apify_result.get("kunde", "Unbekannt"),
                "lieferant": apify_result.get("lieferant", "Unbekannt"),
                "projektnummer": apify_result.get("projektnummer", ""),
                "ordnerstruktur": apify_result.get("ordnerstruktur", ""),
                "dateiname": apify_result.get("dateiname", ""),
                "summe": apify_result.get("summe", ""),
                "datum_dokument": apify_result.get("datum_dokument", ""),
                "notizen": apify_result.get("notizen", ""),
                "zu_pruefen": apify_result.get("zu_pruefen", False),
                
                # Map to Railway format
                "document_type": apify_result.get("dokumenttyp", "general"),
                "intent": self._map_dokumenttyp_to_intent(apify_result.get("dokumenttyp", "")),
                "urgency": "high" if apify_result.get("dringend") else "medium",
                "sentiment": "neutral",  # Apify doesn't provide sentiment
                "summary": apify_result.get("notizen", apify_result.get("anliegen", "")),
                "has_pricing": bool(apify_result.get("summe")),
                "key_topics": [apify_result.get("dokumenttyp", "")],
                "suggested_tasks": [],  # Will be generated by workflow nodes
                "response_needed": apify_result.get("zu_pruefen", False),
                
                # Full Apify response for later use
                "_apify_full": apify_result
            }
            
            state["ai_analysis"] = ai_result
            
            logger.info(f"‚úÖ AI Analysis complete (Apify): {ai_result.get('dokumenttyp')} ‚Üí {ai_result.get('ordnerstruktur', 'N/A')}")
            
        except Exception as e:
            logger.error(f"‚ùå AI Analysis error: {e}")
            state["errors"].append(f"AI analysis failed: {e}")
            state["ai_analysis"] = {"error": str(e), "fallback": True}
        
        return state
    
    async def _workflow_routing_node(self, state: CommunicationState) -> CommunicationState:
        """LangGraph Node: Determine Workflow Path (WEG A/B)"""
        
        contact_found = state.get("contact_match", {}).get("found", False)
        
        if contact_found:
            state["workflow_path"] = "WEG_B"
            logger.info("üéØ Routing to WEG B (Known Contact)")
        else:
            state["workflow_path"] = "WEG_A"
            logger.info("üÜï Routing to WEG A (Unknown Contact)")
        
        return state
    
    def _route_workflow_condition(self, state: CommunicationState) -> str:
        """Conditional edge function for workflow routing"""
        return state["workflow_path"]
    
    async def _weg_a_unknown_contact_node(self, state: CommunicationState) -> CommunicationState:
        """LangGraph Node: WEG A - Unknown Contact Workflow"""
        
        logger.info("üÜï Executing WEG A: Unknown Contact Workflow")
        
        try:
            # üí∞ NEUE FEATURE: Richtpreis-Berechnung auch f√ºr WEG A (unbekannte Kontakte)
            if state.get("message_type") == "call" and state.get('content'):
                try:
                    logger.info(f"üí∞ Calculating price estimate from call transcript (WEG A)...")
                    
                    # Run estimate calculation
                    estimate = calculate_estimate_from_transcript(
                        transcript=state.get('content', ''),
                        caller_info={
                            "name": None,  # Unknown contact
                            "company": None
                        }
                    )
                    
                    if estimate.found:
                        logger.info(f"‚úÖ Price Estimate (WEG A): {estimate.total_cost:.2f} EUR (confidence: {estimate.confidence:.0%})")
                        
                        # Store estimate in state
                        state["price_estimate"] = {
                            "found": True,
                            "project_type": estimate.project_type,
                            "area_sqm": estimate.area_sqm,
                            "material": estimate.material,
                            "work_type": estimate.work_type,
                            "material_cost": estimate.material_cost,
                            "labor_cost": estimate.labor_cost,
                            "additional_cost": estimate.additional_cost,
                            "total_cost": estimate.total_cost,
                            "confidence": estimate.confidence,
                            "calculation_basis": estimate.calculation_basis,
                            "additional_services": estimate.additional_services,
                            "notes": estimate.notes
                        }
                        
                        logger.info(f"‚úÖ Price estimate stored in state for notification")
                    else:
                        logger.info(f"‚ö†Ô∏è No price estimate possible (WEG A): {estimate.notes}")
                        state["price_estimate"] = {
                            "found": False,
                            "notes": estimate.notes
                        }
                
                except Exception as e:
                    logger.error(f"‚ùå Price estimate error (WEG A): {e}")
                    # Continue without estimate - not critical
            
            # Generate employee notification
            notification = await self._generate_employee_notification(state)
            
            # Create temporary CRM entry
            temp_entry = await self._create_temporary_crm_entry(state)
            
            # Generate follow-up tasks
            tasks = [
                AITask(
                    title=f"Unbekannter Kontakt zuordnen: {state['from_contact']}",
                    description=f"Neue {state['message_type']} von unbekanntem Kontakt. Entscheidung √ºber CRM-Aufnahme erforderlich.",
                    assigned_to="sales_team",
                    priority="medium",
                    due_date=self._calculate_due_date(24),  # 24 Stunden
                    task_type="follow_up",
                    contact_id=None
                )
            ]
            
            state["tasks_generated"] = [self._task_to_dict(task) for task in tasks]
            
            logger.info(f"‚úÖ WEG A complete: Generated {len(tasks)} tasks")
            
        except Exception as e:
            logger.error(f"‚ùå WEG A error: {e}")
            state["errors"].append(f"WEG A processing failed: {e}")
        
        return state
    
    async def _weg_b_known_contact_node(self, state: CommunicationState) -> CommunicationState:
        """LangGraph Node: WEG B - Known Contact Workflow"""
        
        logger.info("‚úÖ Executing WEG B: Known Contact Workflow")
        
        try:
            contact_id = state["contact_match"]["contact_id"]
            
            # Update contact communication log
            await self._update_contact_communication_log(contact_id, state)
            
            # Generate AI-suggested tasks
            ai_analysis = state.get("ai_analysis", {})
            tasks = []
            
            for suggested_task in ai_analysis.get("suggested_tasks", []):
                task = AITask(
                    title=suggested_task["title"],
                    description=f"AI-generierte Aufgabe basierend auf {state['message_type']}",
                    assigned_to=self._determine_assignee(suggested_task, state),
                    priority=suggested_task["priority"],
                    due_date=self._calculate_due_date(suggested_task.get("due_hours", 48)),
                    task_type=suggested_task["type"],
                    contact_id=contact_id
                )
                tasks.append(task)
            
            # Create tasks in CRM
            for task in tasks:
                await self._create_crm_task(task)
            
            state["tasks_generated"] = [self._task_to_dict(task) for task in tasks]
            
            # Automatic response if needed
            if ai_analysis.get("response_needed"):
                await self._send_automatic_response(state)
            
            logger.info(f"‚úÖ WEG B complete: Generated {len(tasks)} tasks")
            
        except Exception as e:
            logger.error(f"‚ùå WEG B error: {e}")
            state["errors"].append(f"WEG B processing failed: {e}")
        
        return state
    
    async def _finalize_processing_node(self, state: CommunicationState) -> CommunicationState:
        """LangGraph Node: Finalize Processing"""
        
        logger.info("üèÅ Finalizing AI Communication Processing")
        
        # Mark processing as complete
        state["processing_complete"] = True
        
        # Log final results
        logger.info(f"üìä Processing Summary:")
        logger.info(f"  - Contact Match: {state.get('contact_match', {}).get('found', False)}")
        logger.info(f"  - Workflow Path: {state.get('workflow_path')}")
        logger.info(f"  - Tasks Generated: {len(state.get('tasks_generated', []))}")
        logger.info(f"  - Errors: {len(state.get('errors', []))}")
        
        return state
    
    # ===============================
    # CONTACT LOOKUP METHODS
    # ===============================
    
    async def _search_weclapp_contact(self, contact_identifier: str) -> ContactMatch:
        """
        üîç ENHANCED: Two-Tier Contact Lookup (MASTER PLAN)
        
        STEP 1: SQLite Cache (Sub-Second)
        STEP 2: WeClapp API (only on Cache Miss)
        """
        
        # STEP 1: Check SQLite Cache first
        cached_contact = await lookup_contact_in_cache(contact_identifier)
        
        if cached_contact:
            return ContactMatch(
                found=True,
                contact_id=cached_contact.get("weclapp_contact_id"),
                contact_name=cached_contact.get("contact_name"),  # May be None from minimal cache
                company=cached_contact.get("company_name"),
                confidence=1.0,
                source="cache",
                cache_hits=cached_contact.get("cache_hits", 0)
            )
        
        # STEP 2: Cache Miss - Query WeClapp API
        if not self.weclapp_api_token:
            logger.warning("‚ö†Ô∏è WeClapp API token not configured")
            return ContactMatch(found=False, source="weclapp_unavailable")
        
        try:
            logger.info(f"üîé Cache Miss - Querying WeClapp for: {contact_identifier}")
            
            # WeClapp API Call with EMAIL FILTER for exact match
            headers = {
                "AuthenticationToken": self.weclapp_api_token,
                "Accept": "application/json"
            }
            
            # Determine if contact_identifier is EMAIL or PHONE
            is_phone = contact_identifier.startswith("+") or contact_identifier.isdigit()
            
            # Filter by exact email or phone match using WeClapp API
            if is_phone:
                # Phone number search (try multiple formats)
                search_params = {
                    "phone-eq": contact_identifier,
                    "serializationConfiguration": "IGNORE_EMPTY",
                    "pageSize": 5  # Get multiple results for phone (may match different formats)
                }
                logger.info(f"üîç Searching by PHONE: {contact_identifier}")
            else:
                # Email search
                search_params = {
                    "email-eq": contact_identifier.lower(),
                    "serializationConfiguration": "IGNORE_EMPTY",
                    "pageSize": 1  # Only need 1 result for exact match
                }
                logger.info(f"üîç Searching by EMAIL: {contact_identifier}")
            
            url = f"https://{self.weclapp_domain}.weclapp.com/webapp/api/v1/contact"
            logger.info(f"üìû WeClapp URL: {url} with filter: {search_params}")
            
            async with aiohttp.ClientSession() as session:
                async with session.get(url, headers=headers, params=search_params) as response:
                    logger.info(f"üì¨ WeClapp Response Status: {response.status}")
                    
                    if response.status == 200:
                        data = await response.json()
                        contacts = data.get("result", [])
                        
                        if len(contacts) > 0:
                            # Exact match found via WeClapp email filter
                            contact = contacts[0]
                            contact_name = f"{contact.get('firstName', '')} {contact.get('lastName', '')}".strip()
                            company_name = contact.get("company", {}).get("name") if contact.get("company") else None
                            
                            logger.info(f"‚úÖ EXACT MATCH FOUND in WeClapp: {contact_name} (ID: {contact.get('id')})")
                            
                            # STEP 3: Cache the result for future lookups
                            await cache_contact(contact_identifier, {
                                "weclapp_contact_id": str(contact.get("id")),
                                "weclapp_customer_id": str(contact.get("customerId")) if contact.get("customerId") else None,
                                "contact_name": contact_name,
                                "company_name": company_name,
                                "phone": contact.get("phone")
                            })
                            
                            return ContactMatch(
                                found=True,
                                contact_id=str(contact.get("id")),
                                contact_name=contact_name,
                                company=company_name,
                                confidence=1.0,
                                source="weclapp"
                            )
                        else:
                            logger.warning(f"‚ùå No match found in WeClapp for: {contact_identifier}")
                    else:
                        logger.error(f"‚ùå WeClapp API returned status {response.status}")
            
            return ContactMatch(found=False, source="weclapp")
            
        except Exception as e:
            logger.error(f"‚ùå WeClapp search exception: {e}")
            return ContactMatch(found=False, source="weclapp_error")
    
    async def _search_apify_contact(self, contact_identifier: str) -> ContactMatch:
        """Search contact in Apify datasets"""
        
        # Simplified Apify contact search
        # In production: Search durch Apify datasets
        
        return ContactMatch(found=False, source="apify")
    
    async def _fuzzy_contact_search(self, contact_identifier: str, state: CommunicationState) -> List[Dict[str, Any]]:
        """
        üîç FUZZY CONTACT MATCHING
        
        Erweiterte Suche wenn kein direkter Match:
        1. Domain-Suche (Email) ‚Üí Firma mit mehreren Mitarbeitern
        2. Telefon-Prefix ‚Üí Kunde mit mehreren Nummern  
        3. Namen-Matching ‚Üí Alternativer Kontakt
        """
        
        potential_matches = []
        
        if not self.weclapp_api_token:
            return potential_matches
        
        try:
            headers = {
                "AuthenticationToken": self.weclapp_api_token,
                "Accept": "application/json"
            }
            
            # 1. DOMAIN-SUCHE (Email)
            if "@" in contact_identifier:
                domain = contact_identifier.split("@")[1]
                logger.info(f"üîç Fuzzy: domain @{domain}")
                
                url = f"https://{self.weclapp_domain}.weclapp.com/webapp/api/v1/party"
                async with aiohttp.ClientSession() as session:
                    async with session.get(url, headers=headers, params={"email-like": f"%@{domain}", "pageSize": 5}) as response:
                        if response.status == 200:
                            data = await response.json()
                            for contact in data.get("result", []):
                                if contact.get("email", "").lower() != contact_identifier.lower():
                                    # Safely extract company name (can be string or dict)
                                    company_data = contact.get("company")
                                    company_name = None
                                    if isinstance(company_data, dict):
                                        company_name = company_data.get("name")
                                    elif isinstance(company_data, str):
                                        company_name = company_data
                                    
                                    potential_matches.append({
                                        "match_type": "domain",
                                        "confidence": 0.8,
                                        "contact_id": str(contact.get("id")),
                                        "contact_name": f"{contact.get('firstName', '')} {contact.get('lastName', '')}".strip(),
                                        "company": company_name,
                                        "existing_identifier": contact.get("email"),
                                        "reason": f"Gleiche Firma (@{domain})"
                                    })
            
            # 2. TELEFON-PREFIX
            elif contact_identifier.startswith("+") and len(contact_identifier) >= 8:
                prefix = contact_identifier[:8]
                logger.info(f"üîç Fuzzy: phone {prefix}*")
                
                url = f"https://{self.weclapp_domain}.weclapp.com/webapp/api/v1/party"
                async with aiohttp.ClientSession() as session:
                    async with session.get(url, headers=headers, params={"phone-like": f"{prefix}%", "pageSize": 5}) as response:
                        if response.status == 200:
                            data = await response.json()
                            for contact in data.get("result", []):
                                if contact.get("phone") and contact.get("phone") != contact_identifier:
                                    # Safely extract company name (can be string or dict)
                                    company_data = contact.get("company")
                                    company_name = None
                                    if isinstance(company_data, dict):
                                        company_name = company_data.get("name")
                                    elif isinstance(company_data, str):
                                        company_name = company_data
                                    
                                    potential_matches.append({
                                        "match_type": "phone_prefix",
                                        "confidence": 0.7,
                                        "contact_id": str(contact.get("id")),
                                        "contact_name": f"{contact.get('firstName', '')} {contact.get('lastName', '')}".strip(),
                                        "company": company_name,
                                        "existing_identifier": contact.get("phone"),
                                        "reason": f"√Ñhnliche Nummer ({contact.get('phone')})"
                                    })
            
            # Limit to top 3
            potential_matches = sorted(potential_matches, key=lambda x: x["confidence"], reverse=True)[:3]
            logger.info(f"‚úÖ Fuzzy: {len(potential_matches)} matches")
            return potential_matches
            
        except Exception as e:
            logger.error(f"‚ùå Fuzzy error: {str(e)}")
            return []
    
    # ===============================
    # CRM INTEGRATION METHODS
    # ===============================
    
    async def _update_contact_communication_log(self, contact_id: str, state: CommunicationState):
        """
        üìù WeClapp CRM Communication Log
        
        Creates detailed crmEvent entry with:
        - Call transcript
        - AI Analysis (Intent, Urgency, Sentiment)
        - Generated tasks
        - Call direction (inbound/outbound) & duration
        """
        
        logger.info(f"üìù Creating WeClapp Communication Log for contact {contact_id}")
        
        if not self.weclapp_api_token:
            logger.warning("‚ö†Ô∏è WeClapp API token not configured - skipping CRM log")
            return
        
        try:
            # Prepare communication log data
            message_type = state.get("message_type", "unknown")
            ai_analysis = state.get("ai_analysis", {})
            additional_data = state.get("additional_data", {})
            
            # Call-specific data
            if message_type == "call":
                call_direction = additional_data.get("call_direction", "inbound")
                call_duration = additional_data.get("call_duration", 0)
                call_transcript = state.get("content", "")
                
                description = f"""
üìû **Telefonat ({call_direction.upper()})**

‚è±Ô∏è **Dauer:** {call_duration} Sekunden
üìÖ **Zeitpunkt:** {state.get('timestamp', now_berlin().isoformat())}

---

### üìù Gespr√§chstranskript:
{call_transcript}

---

### ü§ñ KI-Analyse:

**Absicht:** {ai_analysis.get('intent', 'Unbekannt')}
**Dringlichkeit:** {ai_analysis.get('urgency', 'medium')}
**Stimmung:** {ai_analysis.get('sentiment', 'neutral')}

**Zusammenfassung:**
{ai_analysis.get('summary', 'Keine Zusammenfassung verf√ºgbar')}

---

### ‚úÖ Generierte Aufgaben:
"""
                # Add generated tasks to description
                for task in state.get("tasks_generated", []):
                    description += f"\n- {task.get('title', 'Unbekannte Aufgabe')}"
                
                event_type = "CALL"
                subject = f"Telefonat ({call_direction}) - {ai_analysis.get('intent', 'Allgemein')}"
            
            # Email-specific data
            elif message_type == "email":
                description = f"""
üìß **Email empfangen**

üì© **Von:** {state.get('from_contact')}
üìÖ **Zeitpunkt:** {state.get('timestamp', now_berlin().isoformat())}

---

### üìù Email-Inhalt:
{state.get('content', '')[:1000]}...

---

### ü§ñ KI-Analyse:

**Absicht:** {ai_analysis.get('intent', 'Unbekannt')}
**Dringlichkeit:** {ai_analysis.get('urgency', 'medium')}
**Stimmung:** {ai_analysis.get('sentiment', 'neutral')}

---

### ‚úÖ Generierte Aufgaben:
"""
                for task in state.get("tasks_generated", []):
                    description += f"\n- {task.get('title', 'Unbekannte Aufgabe')}"
                
                event_type = "EMAIL"
                subject = f"Email - {ai_analysis.get('intent', 'Allgemein')}"
            
            # WhatsApp-specific data
            elif message_type == "whatsapp":
                description = f"""
üí¨ **WhatsApp Nachricht**

üì± **Von:** {state.get('from_contact')}
üìÖ **Zeitpunkt:** {state.get('timestamp', now_berlin().isoformat())}

---

### üìù Nachricht:
{state.get('content', '')}

---

### ü§ñ KI-Analyse:

**Absicht:** {ai_analysis.get('intent', 'Unbekannt')}
**Dringlichkeit:** {ai_analysis.get('urgency', 'medium')}
**Stimmung:** {ai_analysis.get('sentiment', 'neutral')}

---

### ‚úÖ Generierte Aufgaben:
"""
                for task in state.get("tasks_generated", []):
                    description += f"\n- {task.get('title', 'Unbekannte Aufgabe')}"
                
                event_type = "NOTE"
                subject = f"WhatsApp - {ai_analysis.get('intent', 'Allgemein')}"
            
            else:
                event_type = "NOTE"
                subject = f"{message_type.capitalize()} - Kommunikation"
                description = state.get('content', 'Keine Details verf√ºgbar')
            
            # üéØ NEUE FEATURE: Call Analysis f√ºr Task-Ableitung, Termin-Extraktion, Follow-Ups
            if message_type == "call" and state.get('content'):
                try:
                    call_duration = state.get("call_duration", 0)
                    contact_name = contact_match.get("name", "Unbekannt") if contact_match else "Unbekannt"
                    
                    logger.info(f"üéØ Analyzing call transcript for tasks/appointments...")
                    call_analysis = await analyze_call_content(
                        transcription=state.get('content', ''),
                        contact_name=contact_name,
                        duration_seconds=call_duration
                    )
                    
                    # Log results
                    if call_analysis:
                        logger.info(f"‚úÖ Call Analysis Complete:")
                        logger.info(f"   üìã Tasks: {len(call_analysis.get('tasks', []))}")
                        logger.info(f"   üìÖ Appointments: {len(call_analysis.get('appointments', []))}")
                        logger.info(f"   ‚è∞ Follow-Ups: {len(call_analysis.get('follow_ups', []))}")
                        logger.info(f"   üìù Summary: {call_analysis.get('summary', '')}")
                        
                        # Store analysis in state for later use
                        state["call_analysis"] = call_analysis
                        
                        # Add analyzed tasks to state's tasks_generated
                        for task_data in call_analysis.get('tasks', []):
                            state["tasks_generated"].append({
                                "title": task_data.get("title"),
                                "description": task_data.get("description"),
                                "priority": task_data.get("priority", "medium"),
                                "due_date": task_data.get("due_date"),
                                "source": "call_transcript_analysis"
                            })
                        
                        # Enhance description with analysis
                        description += f"\n\n### üéØ Call-Analyse:\n\n"
                        description += f"**Zusammenfassung:** {call_analysis.get('summary', 'N/A')}\n\n"
                        
                        if call_analysis.get('tasks'):
                            description += "**Erkannte Aufgaben:**\n"
                            for task in call_analysis.get('tasks', []):
                                description += f"- [{task.get('priority', 'medium').upper()}] {task.get('title')}\n"
                        
                        if call_analysis.get('appointments'):
                            description += "\n**Vereinbarte Termine:**\n"
                            for apt in call_analysis.get('appointments', []):
                                description += f"- üìÖ {apt.get('date')} {apt.get('time', '')} - {apt.get('description')}\n"
                        
                        if call_analysis.get('follow_ups'):
                            description += "\n**Follow-Ups:**\n"
                            for followup in call_analysis.get('follow_ups', []):
                                description += f"- ‚è∞ {followup.get('due_date')}: {followup.get('action')}\n"
                    
                except Exception as e:
                    logger.error(f"‚ùå Call analysis error: {e}")
                    # Continue without analysis - not critical
            
            # üí∞ NEUE FEATURE: Richtpreis-Berechnung f√ºr Dacharbeiten aus Transcript
            if message_type == "call" and state.get('content'):
                try:
                    logger.info(f"üí∞ Calculating price estimate from call transcript...")
                    
                    # Run estimate calculation
                    estimate = calculate_estimate_from_transcript(
                        transcript=state.get('content', ''),
                        caller_info={
                            "name": contact_match.get("name") if contact_match else None,
                            "company": contact_match.get("company") if contact_match else None
                        }
                    )
                    
                    if estimate.found:
                        logger.info(f"‚úÖ Price Estimate: {estimate.total_cost:.2f} EUR (confidence: {estimate.confidence:.0%})")
                        
                        # Store estimate in state
                        state["price_estimate"] = {
                            "project_type": estimate.project_type,
                            "area_sqm": estimate.area_sqm,
                            "material": estimate.material,
                            "work_type": estimate.work_type,
                            "material_cost": estimate.material_cost,
                            "labor_cost": estimate.labor_cost,
                            "additional_cost": estimate.additional_cost,
                            "total_cost": estimate.total_cost,
                            "confidence": estimate.confidence,
                            "calculation_basis": estimate.calculation_basis,
                            "additional_services": estimate.additional_services,
                            "notes": estimate.notes
                        }
                        
                        # Add estimate to description
                        description += f"\n\n### üí∞ Automatische Kostensch√§tzung:\n\n"
                        description += f"**Projekt:** {estimate.project_type.title()}\n"
                        description += f"**Fl√§che:** {estimate.area_sqm:.0f} m¬≤\n"
                        description += f"**Material:** {estimate.material}\n"
                        description += f"**Arbeitsart:** {estimate.work_type}\n\n"
                        description += f"**Kosten:**\n"
                        description += f"- Material: {estimate.material_cost:,.2f} EUR\n"
                        description += f"- Arbeit: {estimate.labor_cost:,.2f} EUR\n"
                        if estimate.additional_cost > 0:
                            description += f"- Zusatzleistungen: {estimate.additional_cost:,.2f} EUR\n"
                        description += f"- **GESAMT (Richtwert): {estimate.total_cost:,.2f} EUR**\n\n"
                        description += f"**Genauigkeit:** {estimate.confidence:.0%}\n"
                        description += f"_{estimate.notes}_\n"
                        
                        # Create automatic task for quote preparation
                        state["tasks_generated"].append({
                            "title": f"Angebot vorbereiten: {estimate.project_type.title()} ({estimate.area_sqm:.0f} m¬≤)",
                            "description": f"Richtpreis: {estimate.total_cost:,.2f} EUR\n\nBasis:\n" + "\n".join(f"- {basis}" for basis in estimate.calculation_basis),
                            "priority": "high" if estimate.total_cost > 10000 else "medium",
                            "due_date": (now_berlin() + timedelta(days=2)).isoformat(),
                            "source": "price_estimate"
                        })
                        
                        logger.info(f"‚úÖ Price estimate added to communication log and task generated")
                    else:
                        logger.info(f"‚ö†Ô∏è No price estimate possible: {estimate.notes}")
                        # Still store the reason in state
                        state["price_estimate"] = {
                            "found": False,
                            "notes": estimate.notes
                        }
                
                except Exception as e:
                    logger.error(f"‚ùå Price estimate error: {e}")
                    # Continue without estimate - not critical
            
            # Create WeClapp crmEvent
            # Determine correct type based on message type and direction
            if message_type == "call":
                call_direction = state.get("call_direction", "inbound")
                crm_type = "INCOMING_CALL" if call_direction == "inbound" else "OUTGOING_CALL"
            elif message_type == "email":
                crm_type = "LETTER"  # WeClapp uses LETTER for emails
            else:
                crm_type = "GENERAL"
            
            crm_event_data = {
                "partyId": int(contact_id),
                "contactId": int(contact_id),  # ‚úÖ For PERSON parties, contactId must equal partyId
                "type": crm_type,  # ‚úÖ INCOMING_CALL, OUTGOING_CALL, LETTER, or GENERAL
                "eventType": event_type,
                "subject": subject,
                "description": description,
                "contactChannel": "PHONE" if message_type == "call" else "EMAIL" if message_type == "email" else "OTHER",
                "status": "DONE",
                "eventDate": int(now_berlin().timestamp() * 1000)  # Unix timestamp in milliseconds
            }
            
            headers = {
                "AuthenticationToken": self.weclapp_api_token,
                "Content-Type": "application/json",
                "Accept": "application/json"
            }
            
            url = f"https://{self.weclapp_domain}.weclapp.com/webapp/api/v1/crmEvent"
            
            logger.info(f"üì§ Creating WeClapp crmEvent: {subject}")
            
            async with aiohttp.ClientSession() as session:
                async with session.post(url, headers=headers, json=crm_event_data) as response:
                    if response.status == 201:
                        crm_event = await response.json()
                        logger.info(f"‚úÖ WeClapp crmEvent created: ID {crm_event.get('id')}")
                        return crm_event
                    else:
                        error_text = await response.text()
                        logger.error(f"‚ùå WeClapp crmEvent creation failed: {response.status} - {error_text}")
                        return None
        
        except Exception as e:
            logger.error(f"‚ùå WeClapp Communication Log error: {str(e)}")
            import traceback
            logger.error(traceback.format_exc())
            return None
        
    async def _create_crm_task(self, task: AITask):
        """Create task in CRM system"""
        
        logger.info(f"üìã Creating CRM task: {task.title}")
        
        # WeClapp task creation
        # Implementierung abh√§ngig von WeClapp API
    
    async def _create_temporary_crm_entry(self, state: CommunicationState) -> Dict[str, Any]:
        """Create temporary CRM entry for unknown contact"""
        
        logger.info(f"üÜï Creating temporary CRM entry for {state['from_contact']}")
        
        return {
            "temp_id": f"temp_{now_berlin().timestamp()}",
            "contact": state["from_contact"],
            "status": "pending_assignment"
        }
    
    # ===============================
    # NOTIFICATION METHODS
    # ===============================
    
    async def _generate_employee_notification(self, state: CommunicationState) -> Dict[str, Any]:
        """Generate notification for employee decision"""
        
        logger.info("üìß Generating employee notification")
        
        notification = {
            "type": "unknown_contact_decision",
            "subject": f"Neue {state['message_type']} von unbekanntem Kontakt: {state['from_contact']}",
            "content": f"""
Neue Kommunikation von unbekanntem Kontakt ben√∂tigt Entscheidung:

üìß **Details:**
- Von: {state['from_contact']}
- Art: {state['message_type']}
- Zeit: {state['timestamp']}
- Inhalt: {state['content'][:300]}...

ü§ñ **AI-Analyse:**
{json.dumps(state.get('ai_analysis', {}), indent=2, ensure_ascii=False)}

üéØ **Erforderliche Aktion:**
- [ ] Neuen Kontakt im CRM anlegen
- [ ] Als privat/Spam markieren
- [ ] Weitere Informationen einholen

Antworten Sie mit den erforderlichen Kontakt-Details oder markieren Sie als "Privat/Spam".
""",
            "priority": state.get("ai_analysis", {}).get("urgency", "medium"),
            "assigned_to": self._determine_responsible_employee(state)
        }
        
        return notification
    
    async def _send_automatic_response(self, state: CommunicationState):
        """Send automatic response if needed"""
        
        logger.info("ü§ñ Sending automatic response")
        
        # Automatische Antwort-Logik
        # Je nach message_type unterschiedliche Antworten
    
    # ===============================
    # UTILITY METHODS
    # ===============================
    
    def _determine_assignee(self, suggested_task: Dict[str, Any], state: CommunicationState) -> str:
        """Determine who should be assigned to a task"""
        
        task_type = suggested_task.get("type", "follow_up")
        
        mapping = {
            "sales": "sales_team",
            "support": "support_team", 
            "quote": "sales_team",
            "meeting": "management",
            "follow_up": "sales_team"
        }
        
        return mapping.get(task_type, "general_team")
    
    def _determine_responsible_employee(self, state: CommunicationState) -> str:
        """Determine responsible employee for unknown contact"""
        
        # Zeit-basierte Zuweisung
        hour = now_berlin().hour
        
        if 9 <= hour <= 17:
            return "sales_team"
        else:
            return "service_team"
    
    def _calculate_due_date(self, hours_from_now: int) -> str:
        """Calculate due date"""
        
        from datetime import datetime, timedelta
        
        due_date = now_berlin() + timedelta(hours=hours_from_now)
        return due_date.isoformat()
    
    def _task_to_dict(self, task: AITask) -> Dict[str, Any]:
        """Convert AITask to dictionary"""
        
        return {
            "title": task.title,
            "description": task.description,
            "assigned_to": task.assigned_to,
            "priority": task.priority,
            "due_date": task.due_date,
            "task_type": task.task_type,
            "contact_id": task.contact_id
        }
    
    # ===============================
    # MAIN PROCESSING METHOD
    # ===============================
    
    async def process_communication(self, 
                                  message_type: str,
                                  from_contact: str,
                                  content: str,
                                  additional_data: Dict[str, Any] = None) -> Dict[str, Any]:
        """Main processing method - executes the LangGraph workflow"""
        
        logger.info(f"üöÄ Processing {message_type} from {from_contact}")
        
        # Initialize state
        initial_state = CommunicationState(
            message_type=message_type,
            from_contact=from_contact,
            content=content,
            timestamp=now_berlin().isoformat(),
            contact_match=None,
            ai_analysis=None,
            workflow_path=None,
            tasks_generated=[],
            crm_updates=[],
            processing_complete=False,
            response_sent=False,
            errors=[],
            additional_data=additional_data or {}
        )
        
        try:
            # Execute LangGraph workflow
            final_state = await self.workflow.ainvoke(initial_state)
            
            # Build processing result
            # Get additional_data from final_state (it flows through the workflow)
            state_additional_data = final_state.get("additional_data", {})
            
            processing_result = {
                "success": True,
                "workflow_path": final_state.get("workflow_path"),
                "contact_match": final_state.get("contact_match"),
                "ai_analysis": final_state.get("ai_analysis"),
                "tasks_generated": final_state.get("tasks_generated", []),
                "processing_complete": final_state.get("processing_complete", False),
                "errors": final_state.get("errors", []),
                # Attachment info from state's additional_data
                "attachments_count": state_additional_data.get("attachments_count", 0),
                "has_attachments": state_additional_data.get("has_attachments", False),
                "attachment_results": state_additional_data.get("attachment_results", []),
                # üí∞ Price Estimate (if calculated)
                "price_estimate": final_state.get("price_estimate"),
                # üìä DETAILED PROCESSING INFO (for debugging/monitoring)
                "processing_details": {
                    "timestamp": final_state.get("timestamp"),
                    "message_type": message_type,
                    "from_contact": from_contact,
                    "workflow_executed": final_state.get("workflow_path"),
                    "contact_lookup": {
                        "attempted": True,
                        "found": final_state.get("contact_match", {}).get("found", False),
                        "source": final_state.get("contact_match", {}).get("source", "none"),
                        "contact_id": final_state.get("contact_match", {}).get("contact_id"),
                        "contact_name": final_state.get("contact_match", {}).get("name"),
                        "company": final_state.get("contact_match", {}).get("company")
                    },
                    "ai_processing": {
                        "executed": final_state.get("ai_analysis") is not None,
                        "intent": final_state.get("ai_analysis", {}).get("intent"),
                        "urgency": final_state.get("ai_analysis", {}).get("urgency"),
                        "sentiment": final_state.get("ai_analysis", {}).get("sentiment")
                    },
                    "pricing": {
                        "attempted": message_type == "call" and bool(content),
                        "calculated": final_state.get("price_estimate", {}).get("found", False) if final_state.get("price_estimate") else False,
                        "amount": final_state.get("price_estimate", {}).get("total_cost") if final_state.get("price_estimate") else None,
                        "confidence": final_state.get("price_estimate", {}).get("confidence") if final_state.get("price_estimate") else None
                    },
                    "tasks": {
                        "count": len(final_state.get("tasks_generated", [])),
                        "types": [task.get("task_type") for task in final_state.get("tasks_generated", [])]
                    },
                    "crm_updates": {
                        "count": len(final_state.get("crm_updates", [])),
                        "types": [update.get("type") for update in final_state.get("crm_updates", [])]
                    },
                    "attachments": {
                        "count": state_additional_data.get("attachments_count", 0),
                        "processed": len(state_additional_data.get("attachment_results", []))
                    }
                }
            }
            
            # üéØ SEND FINAL ZAPIER NOTIFICATION
            try:
                notification_sent = await send_final_notification(
                    processing_result, message_type, from_contact, content
                )
                processing_result["notification_sent"] = notification_sent
                if notification_sent:
                    logger.info("‚úÖ Final email notification sent via Zapier")
                else:
                    logger.warning("‚ö†Ô∏è Final email notification failed")
            except Exception as notification_error:
                logger.error(f"‚ùå Notification error: {notification_error}")
                processing_result["notification_sent"] = False
                processing_result["notification_error"] = str(notification_error)
            
            # üíæ SAVE TO DATABASE
            db_saved = False
            db_record_id = None
            db_error_msg = None
            try:
                message_id = additional_data.get("message_id", "") if additional_data else ""
                db_record_id = await save_to_database(processing_result, message_type, from_contact, content, final_state, message_id)
                db_saved = True
                logger.info(f"‚úÖ Data saved to database (record_id: {db_record_id})")
            except Exception as db_error:
                logger.error(f"‚ùå Database save error: {db_error}")
                db_error_msg = str(db_error)
                # Don't fail the whole process if DB save fails
            
            # üìù ADD DATABASE & NOTIFICATION INFO TO RESPONSE
            processing_result["processing_details"]["database"] = {
                "saved": db_saved,
                "record_id": db_record_id,
                "error": db_error_msg
            }
            processing_result["processing_details"]["notification"] = {
                "sent": processing_result.get("notification_sent", False),
                "error": processing_result.get("notification_error")
            }
            
            return processing_result
            
        except Exception as e:
            logger.error(f"‚ùå Workflow execution error: {e}")
            error_result = {
                "success": False,
                "error": str(e),
                "workflow_path": None,
                "processing_complete": False,
                "notification_sent": False
            }
            
            # Send error notification too
            try:
                await send_final_notification(error_result, message_type, from_contact, content)
            except:
                pass  # Don't fail on notification error
                
            return error_result

# ===============================
# FASTAPI PRODUCTION SERVER
# ===============================

# Initialize SQLite Contact Cache
initialize_contact_cache()

# Initialize Orchestrator
orchestrator = ProductionAIOrchestrator()

# FastAPI App with startup/shutdown events
@asynccontextmanager
async def lifespan(app: FastAPI):
    """Lifecycle manager for FastAPI application"""
    # STARTUP
    logger.info("üöÄ Starting AI Communication Orchestrator...")
    logger.info("üì• Downloading WEClapp Sync Database from OneDrive...")
    
    try:
        # Pre-download WEClapp DB at startup
        db_available = await ensure_weclapp_db_available()
        if db_available:
            logger.info("‚úÖ WEClapp Sync DB ready at startup")
        else:
            logger.warning("‚ö†Ô∏è WEClapp Sync DB not available - will retry on first request")
    except Exception as e:
        logger.error(f"‚ùå Startup WEClapp DB download error: {e}")
    
    logger.info("‚úÖ AI Communication Orchestrator ready!")
    
    yield  # Server is running
    
    # SHUTDOWN
    logger.info("üëã Shutting down AI Communication Orchestrator...")

app = FastAPI(
    title="AI Communication Orchestrator",
    description="Production-ready LangGraph AI Communication Processing System",
    version="1.3.0-weclapp-sync",
    lifespan=lifespan
)

# CORS Middleware
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

@app.get("/")
async def root():
    """Health check endpoint"""
    weclapp_db_status = "‚úÖ Available" if os.path.exists("/tmp/weclapp_sync.db") else "‚ö†Ô∏è Not Downloaded"
    
    return {
        "status": "‚úÖ AI Communication Orchestrator ONLINE",
        "system": "LangGraph + FastAPI Production",
        "version": "1.4.0-sipgate-pricing",
        "weclapp_sync_db": weclapp_db_status,
        "endpoints": [
            "/webhook/ai-email (deprecated - use /incoming or /outgoing)",
            "/webhook/ai-email/incoming",
            "/webhook/ai-email/outgoing",
            "/webhook/ai-email/test (test with JSON attachments)",
            "/webhook/ai-call",
            "/webhook/frontdesk",
            "/webhook/feedback",
            "/webhook/ai-whatsapp"
        ],
        "features": [
            "Email Direction Detection (incoming/outgoing)",
            "Document Type Classification (invoice/offer/order/delivery/general)",
            "Intelligent Attachment Processing",
            "Type-specific OCR Routes",
            "üí∞ Automatic Price Estimation from Call Transcripts (NEW)",
            "Database Schema Migration Support (NEW)",
            "üß™ Test Endpoint with JSON Attachments (NEW)"
        ],
        "timestamp": now_berlin().isoformat()
    }

@app.post("/webhook/ai-email/test")
async def process_email_test(request: Request):
    """
    üß™ TEST EMAIL PROCESSING WITH JSON ATTACHMENTS
    
    For testing purposes - accepts complete email data in JSON format
    including attachments metadata (simulated, no actual file download)
    
    Expected payload:
    {
        "from": "sender@example.com",
        "to": "recipient@example.com",
        "subject": "Email Subject",
        "body": "Email content...",
        "attachments": [
            {
                "filename": "document.pdf",
                "size": 145000,
                "content_type": "application/pdf"
            }
        ],
        "received_date": "2025-10-16T18:00:00+02:00"
    }
    """
    
    try:
        data = await request.json()
        logger.info(f"üß™ TEST: Processing email from {data.get('from', 'unknown')}")
        
        # Extract data
        from_contact = data.get("from", "")
        to_contact = data.get("to", "")
        subject = data.get("subject", "")
        body = data.get("body", "")
        attachments_meta = data.get("attachments", [])
        received_date = data.get("received_date", now_berlin().isoformat())
        
        # Simulate attachment results (since we can't actually download in test)
        attachment_results = []
        for att in attachments_meta:
            filename = att.get("filename", "unknown.pdf")
            size = att.get("size", 0)
            content_type = att.get("content_type", "application/pdf")
            
            # Classify document type from filename
            doc_type = "general"
            if any(word in filename.lower() for word in ["rechnung", "invoice", "re-"]):
                doc_type = "invoice"
            elif any(word in filename.lower() for word in ["angebot", "offer", "quote"]):
                doc_type = "offer"
            elif any(word in filename.lower() for word in ["bestellung", "order", "po-"]):
                doc_type = "order"
            elif any(word in filename.lower() for word in ["liefer", "delivery", "dn-"]):
                doc_type = "delivery_note"
            
            # Simulate OCR result (test placeholder)
            attachment_results.append({
                "filename": filename,
                "size": size,
                "content_type": content_type,
                "document_type": doc_type,
                "ocr_route": f"simulated_{doc_type}_ocr",
                "ocr_text": f"[TEST: Simulated OCR for {filename} - Type: {doc_type}]",
                "structured_data": {
                    "test_mode": True,
                    "document_type": doc_type
                }
            })
        
        logger.info(f"üß™ TEST: Simulated {len(attachment_results)} attachments")
        
        # Process via LangGraph with simulated attachments
        result = await orchestrator.process_communication(
            message_type="email",
            from_contact=from_contact,
            content=f"{subject}\n\n{body}",
            additional_data={
                "subject": subject,
                "body": body,
                "to": to_contact,
                "received_date": received_date,
                "attachments_count": len(attachment_results),
                "has_attachments": len(attachment_results) > 0,
                "attachment_results": attachment_results,
                "test_mode": True
            }
        )
        
        logger.info(f"‚úÖ TEST: Email processing complete: {result.get('workflow_path', 'unknown')}")
        
        return {
            "status": "success",
            "test_mode": True,
            "ai_processing": result,
            "simulated_attachments": len(attachment_results)
        }
        
    except Exception as e:
        logger.error(f"‚ùå TEST: Email processing error: {e}")
        import traceback
        logger.error(traceback.format_exc())
        return JSONResponse(
            status_code=500,
            content={"status": "error", "error": str(e), "test_mode": True}
        )

@app.post("/webhook/ai-email")
@app.post("/webhook/ai-email/incoming")
async def process_email_incoming(request: Request):
    """
    üìß INCOMING EMAIL PROCESSING WITH MICROSOFT GRAPH API
    
    Zapier sends minimal metadata ‚Üí Railway loads full email via Graph API
    
    Expected payload from Zapier:
    {
        "message_id": "AAMkAGE1...",  # Graph API Message ID
        "user_email": "mj@cdtechnologies.de",  # Mailbox to query
        "from": "sender@example.com",  # Optional metadata
        "subject": "...",  # Optional metadata
        "document_type_hint": "invoice",  # Optional: invoice|offer|order_confirmation|delivery_note|general
        "priority": "high"  # Optional: low|medium|high|urgent
    }
    """
    
    try:
        data = await request.json()
        data["email_direction"] = "incoming"  # Mark as incoming
        message_id = data.get("message_id") or data.get("id")
        user_email = data.get("user_email") or data.get("mailbox") or data.get("recipient")
        
        # üéØ NEW: Extract document_type_hint from Zapier (Multi-Zap Strategy)
        document_type_hint = data.get("document_type_hint")
        priority = data.get("priority", "medium")
        
        # ‚ö° IMMEDIATE RESPONSE - No logging before response!
        # Fire-and-forget background task
        import asyncio
        asyncio.create_task(process_email_background(
            data, message_id, user_email, 
            document_type_hint=document_type_hint,
            priority=priority
        ))
        
        # Return immediately (< 1 second)
        return JSONResponse(
            status_code=200,
            content={
                "status": "accepted",
                "message_id": message_id,
                "direction": "incoming"
            }
        )
        
    except Exception as e:
        # Even errors return fast
        return JSONResponse(
            status_code=200,
            content={"status": "error", "error": str(e)}
        )

@app.post("/webhook/ai-email/outgoing")
async def process_email_outgoing(request: Request):
    """
    üì§ OUTGOING EMAIL PROCESSING WITH MICROSOFT GRAPH API
    
    Zapier sends minimal metadata ‚Üí Railway loads full email via Graph API
    
    Expected payload from Zapier:
    {
        "message_id": "AAMkAGE1...",  # Graph API Message ID
        "user_email": "mj@cdtechnologies.de",  # Mailbox to query
        "to": "recipient@example.com",  # Optional metadata
        "subject": "...",  # Optional metadata
        "document_type_hint": "invoice",  # Optional
        "priority": "medium"  # Optional
    }
    """
    
    try:
        data = await request.json()
        data["email_direction"] = "outgoing"  # Mark as outgoing
        message_id = data.get("message_id") or data.get("id")
        user_email = data.get("user_email") or data.get("mailbox") or data.get("sender")
        
        # üéØ NEW: Extract document_type_hint from Zapier
        document_type_hint = data.get("document_type_hint")
        priority = data.get("priority", "medium")
        
        # ‚ö° IMMEDIATE RESPONSE - No logging before response!
        # Fire-and-forget background task
        import asyncio
        asyncio.create_task(process_email_background(
            data, message_id, user_email,
            document_type_hint=document_type_hint,
            priority=priority
        ))
        
        # Return immediately (< 1 second)
        return JSONResponse(
            status_code=200,
            content={
                "status": "accepted",
                "message_id": message_id,
                "direction": "outgoing"
            }
        )
        
    except Exception as e:
        # Even errors return fast
        return JSONResponse(
            status_code=200,
            content={"status": "error", "error": str(e)}
        )

async def process_attachments_intelligent(
    attachments: List[Dict],
    message_id: str,
    user_email: str,
    access_token: str,
    subject: str,
    document_type_hint: str = None  # üéØ NEW: Accept hint from Zapier
) -> List[Dict]:
    """
    üìé INTELLIGENT ATTACHMENT PROCESSING
    
    1. Classify document type from subject + filename (or use hint from Zapier)
    2. Download attachment bytes from Graph API
    3. Choose OCR route based on type:
       - invoice ‚Üí PDF.co Invoice Parser
       - delivery_note/aufma√ü ‚Üí PDF.co Handwriting OCR
       - offer/order ‚Üí PDF.co Standard OCR
    4. Extract text and structured data
    5. Return results for GPT analysis
    """
    results = []
    
    try:
        import httpx
        import base64
        
        # üéØ PRIORITY 1: Use document_type_hint from Zapier (Multi-Zap Strategy)
        if document_type_hint:
            expected_type = document_type_hint
            logger.info(f"üéØ FAST-PATH: Using document type hint from Zapier: {expected_type}")
        else:
            # FALLBACK: Classify expected document type from subject
            subject_lower = subject.lower()
            expected_type = "general"
            
            if any(word in subject_lower for word in ["rechnung", "invoice", "re:"]):
                expected_type = "invoice"
            elif any(word in subject_lower for word in ["angebot", "offer", "quote"]):
                expected_type = "offer"
            elif any(word in subject_lower for word in ["auftragsbest√§tigung", "ab:", "order"]):
                expected_type = "order_confirmation"
            elif any(word in subject_lower for word in ["aufma√ü", "lieferschein", "delivery"]):
                expected_type = "delivery_note"
            
            logger.info(f"üìä Expected document type from subject: {expected_type}")
        
        for attachment in attachments:
            try:
                att_id = attachment.get("id")
                att_name = attachment.get("name", "unknown")
                att_type = attachment.get("contentType", "")
                att_size = attachment.get("size", 0)
                
                logger.info(f"üìé Processing: {att_name} ({att_type}, {att_size} bytes)")
                
                # Only process PDFs and images
                if att_type not in ["application/pdf", "image/jpeg", "image/png", "image/jpg"]:
                    logger.warning(f"‚ö†Ô∏è Skipping unsupported type: {att_type}")
                    continue
                
                # Download attachment bytes via Graph API
                download_url = f"https://graph.microsoft.com/v1.0/users/{user_email}/messages/{message_id}/attachments/{att_id}/$value"
                
                async with httpx.AsyncClient(timeout=30.0) as client:
                    response = await client.get(
                        download_url,
                        headers={"Authorization": f"Bearer {access_token}"}
                    )
                    
                    if response.status_code == 200:
                        file_bytes = response.content
                        logger.info(f"‚úÖ Downloaded {len(file_bytes)} bytes for {att_name}")
                        
                        # üîê Calculate SHA256 hash for duplicate detection
                        import hashlib
                        document_hash = hashlib.sha256(file_bytes).hexdigest()
                        logger.info(f"üîê Document hash: {document_hash[:16]}...")
                        
                        # Build email_data dict for OCR function
                        email_data_dict = {
                            "user_email": user_email,
                            "message_id": message_id,
                            "access_token": access_token
                        }
                        
                        # Choose OCR route based on type
                        ocr_result = await process_attachment_ocr(
                            file_bytes=file_bytes,
                            filename=att_name,
                            content_type=att_type,
                            document_type=expected_type,
                            email_data=email_data_dict,
                            attachment=attachment
                        )
                        
                        results.append({
                            "filename": att_name,
                            "type": att_type,
                            "size": att_size,
                            "file_bytes": file_bytes,  # üíæ Keep for OneDrive upload
                            "document_hash": document_hash,  # üîê For duplicate detection
                            "document_type": expected_type,
                            "ocr_text": ocr_result.get("text", ""),
                            "structured_data": ocr_result.get("structured", {}),
                            "ocr_route": ocr_result.get("route", "none")
                        })
                        
                    else:
                        logger.error(f"‚ùå Failed to download {att_name}: {response.status_code}")
                        
            except Exception as att_error:
                logger.error(f"‚ùå Error processing attachment {att_name}: {att_error}")
                
    except Exception as e:
        logger.error(f"‚ùå Attachment processing error: {e}")
    
    return results


async def process_attachment_ocr(
    file_bytes: bytes,
    filename: str,
    content_type: str,
    document_type: str,
    email_data: Dict = None,
    attachment: Dict = None
) -> Dict:
    """
    ü§ñ OCR PROCESSING WITH TYPE-SPECIFIC ROUTES
    
    Routes:
    - invoice ‚Üí PDF.co Invoice Parser (structured extraction)
    - delivery_note ‚Üí PDF.co Handwriting OCR
    - offer/order/general ‚Üí PDF.co Standard OCR
    """
    result = {"text": "", "structured": {}, "route": "none"}
    
    try:
        import httpx
        import base64
        
        pdfco_api_key = os.getenv("PDFCO_API_KEY")
        if not pdfco_api_key:
            logger.warning("‚ö†Ô∏è PDFCO_API_KEY not found, using placeholder")
            result["route"] = f"{document_type}_placeholder"
            result["text"] = f"[OCR Placeholder - PDFCO_API_KEY missing]"
            return result
        
        logger.info(f"üîç OCR Route: {document_type} for {filename}")
        
        # Convert file_bytes to base64
        file_base64 = base64.b64encode(file_bytes).decode('utf-8')
        
        # Choose PDF.co route based on document type
        if document_type == "invoice":
            # PDF.co Invoice Parser - Upload file first, then parse
            logger.info("üìä Using PDF.co Invoice Parser...")
            
            try:
                async with httpx.AsyncClient(timeout=60.0) as client:
                    # Step 1: Upload file to PDF.co
                    upload_response = await client.post(
                        "https://api.pdf.co/v1/file/upload",
                        headers={"x-api-key": pdfco_api_key},
                        json={
                            "name": filename,
                            "file": file_base64
                        }
                    )
                    
                    if upload_response.status_code != 200:
                        error_text = upload_response.text if upload_response.text else "No error message"
                        result["text"] = f"[OCR Error: Upload failed - HTTP {upload_response.status_code}]"
                        result["route"] = "invoice_ocr_failed"
                        logger.warning(f"‚ö†Ô∏è PDF.co upload error: {upload_response.status_code}")
                        logger.warning(f"‚ö†Ô∏è PDF.co Response: {error_text[:500]}")
                        return result
                    
                    upload_data = upload_response.json()
                    if upload_data.get("error"):
                        result["text"] = f"[OCR Error: {upload_data.get('message')}]"
                        result["route"] = "invoice_ocr_failed"
                        logger.warning(f"‚ö†Ô∏è PDF.co upload error: {upload_data.get('message')}")
                        return result
                    
                    uploaded_url = upload_data.get("url")
                    logger.info(f"‚úÖ File uploaded to PDF.co: {uploaded_url[:100]}...")
                    
                    # Step 2: Call AI Invoice Parser with uploaded file URL
                    response = await client.post(
                        "https://api.pdf.co/v1/ai-invoice-parser",
                        headers={"x-api-key": pdfco_api_key},
                        json={"url": uploaded_url}
                    )
                    
                    if response.status_code == 200:
                        data = response.json()
                        if data.get("error") == False:
                            job_id = data.get("jobId")
                            logger.info(f"‚úÖ Invoice parser job created: {job_id}")
                            
                            # Poll for results
                            for attempt in range(10):
                                await asyncio.sleep(3)
                                job_response = await client.post(
                                    "https://api.pdf.co/v1/job/check",
                                    headers={"x-api-key": pdfco_api_key},
                                    json={"jobid": job_id}
                                )
                                
                                if job_response.status_code == 200:
                                    job_data = job_response.json()
                                    if job_data.get("status") == "success":
                                        body = job_data.get("body", {})
                                        invoice_data = body.get("invoice") or body.get("header") or {}
                                        vendor = body.get("vendor") or {}
                                        
                                        result["text"] = json.dumps(body, ensure_ascii=False)[:500]
                                        result["structured"] = {
                                            "invoice_number": invoice_data.get("invoiceNumber") or invoice_data.get("invoice_number", ""),
                                            "total_amount": invoice_data.get("total") or invoice_data.get("totalAmount", ""),
                                            "vendor_name": vendor.get("name") or vendor.get("companyName", ""),
                                            "invoice_date": invoice_data.get("date") or invoice_data.get("invoiceDate", ""),
                                            "due_date": invoice_data.get("dueDate", "")
                                        }
                                        result["route"] = "invoice_ocr"
                                        logger.info(f"‚úÖ Invoice parsed: {result['structured'].get('invoice_number', 'N/A')}")
                                        break
                                    elif job_data.get("status") == "error":
                                        result["text"] = f"[OCR Error: {job_data.get('error')}]"
                                        result["route"] = "invoice_ocr_failed"
                                        logger.warning(f"‚ö†Ô∏è Invoice OCR failed: {job_data.get('error')}")
                                        break
                        else:
                            result["text"] = f"[OCR Error: {data.get('message')}]"
                            result["route"] = "invoice_ocr_failed"
                            logger.warning(f"‚ö†Ô∏è Invoice parser error: {data.get('message')}")
                    else:
                        error_text = response.text if response.text else "No error message"
                        result["text"] = f"[OCR Error: HTTP {response.status_code} - {error_text[:200]}]"
                        result["route"] = "invoice_ocr_failed"
                        logger.warning(f"‚ö†Ô∏è Invoice parser HTTP error: {response.status_code}")
                        logger.warning(f"‚ö†Ô∏è PDF.co Response: {error_text[:500]}")
                        
            except Exception as e:
                logger.error(f"‚ùå Invoice OCR exception: {e}")
                result["text"] = f"[OCR Error: {str(e)}]"
                result["route"] = "invoice_ocr_failed"
        
        elif document_type == "delivery_note":
            # PDF.co Handwriting OCR
            logger.info("‚úçÔ∏è Using PDF.co Handwriting OCR...")
            async with httpx.AsyncClient(timeout=60.0) as client:
                response = await client.post(
                    "https://api.pdf.co/v1/pdf/convert/to/text",
                    headers={"x-api-key": pdfco_api_key},
                    json={
                        "file": file_base64,
                        "inline": True,
                        "async": False,
                        "OCRLanguage": "German",
                        "OCRMode": "TextFromImagesAndVectorGraphicsAndText"
                    }
                )
                
                if response.status_code == 200:
                    data = response.json()
                    if data.get("error") == False:
                        result["text"] = data.get("body", "")[:500]
                        result["route"] = "handwriting_ocr"
                        logger.info(f"‚úÖ Handwriting OCR: {len(result['text'])} chars")
                    else:
                        result["text"] = f"[OCR Error: {data.get('message')}]"
                        result["route"] = "handwriting_ocr_failed"
        
        else:
            # PDF.co Standard OCR
            logger.info("üìÑ Using PDF.co Standard OCR...")
            async with httpx.AsyncClient(timeout=60.0) as client:
                response = await client.post(
                    "https://api.pdf.co/v1/pdf/convert/to/text",
                    headers={"x-api-key": pdfco_api_key},
                    json={
                        "file": file_base64,
                        "inline": True,
                        "async": False,
                        "OCRLanguage": "German"
                    }
                )
                
                if response.status_code == 200:
                    data = response.json()
                    if data.get("error") == False:
                        result["text"] = data.get("body", "")[:500]
                        result["route"] = "standard_ocr"
                        logger.info(f"‚úÖ Standard OCR: {len(result['text'])} chars")
                    else:
                        result["text"] = f"[OCR Error: {data.get('message')}]"
                        result["route"] = "standard_ocr_failed"
        
    except Exception as e:
        logger.error(f"‚ùå OCR error for {filename}: {e}")
        result["text"] = f"[OCR Exception: {str(e)}]"
        result["route"] = "ocr_exception"
    
    return result


async def process_email_background(
    data: dict, 
    message_id: str, 
    user_email: str, 
    document_type_hint: str = None, 
    priority: str = "medium"
):
    """
    Background task to process email without blocking Zapier webhook
    
    Args:
        data: Full webhook payload from Zapier
        message_id: Microsoft Graph API message ID
        user_email: Mailbox email (mj@cdtechnologies.de)
        document_type_hint: Optional hint from Zapier (invoice|offer|order_confirmation|delivery_note|general)
        priority: Optional priority from Zapier (low|medium|high|urgent)
    """
    try:
        # NOW we can log (after response sent to Zapier)
        logger.info(f"üìß Email webhook: message_id={message_id}, user={user_email}")
        
        if document_type_hint:
            logger.info(f"üéØ FAST-PATH: Document type hint from Zapier: {document_type_hint} (priority: {priority})")
        
        logger.info(f"üîç DEBUG: Full data keys: {list(data.keys())}")
        
        # If message_id provided ‚Üí Load full email from Graph API
        if message_id and user_email:
            logger.info(f"üîç Loading full email via Graph API: message_id={message_id}, mailbox={user_email}")
            
            # Get Graph API token
            access_token = await get_graph_token_mail()
            if not access_token:
                logger.error("‚ùå Failed to get Graph API token")
                return
            
            # Fetch full email with attachments
            email_data = await fetch_email_details_with_attachments(
                user_email=user_email,
                message_id=message_id,
                access_token=access_token
            )
            
            if not email_data:
                logger.error(f"‚ùå Failed to load email from Graph API: {message_id}")
                return
            
            # Extract email details
            from_address = email_data.get("from", {}).get("emailAddress", {}).get("address", "")
            subject = email_data.get("subject", "")
            body = email_data.get("body", {}).get("content", "")
            body_type = email_data.get("body", {}).get("contentType", "html")
            attachments = email_data.get("attachments", [])
            
            logger.info(f"‚úÖ Email loaded: From={from_address}, Subject={subject}, Attachments={len(attachments)}")
            
            # ÔøΩ SCHRITT 1: DUPLIKATPR√úFUNG VOR PROCESSING
            tracking_db = get_email_tracking_db()
            processing_start_time = asyncio.get_event_loop().time()
            
            # Check 1: Message ID bereits verarbeitet?
            duplicate_by_id = tracking_db.check_duplicate_by_message_id(message_id)
            if duplicate_by_id:
                logger.warning(f"‚ö†Ô∏è DUPLICATE by Message ID: {message_id}")
                logger.warning(f"   Original: {duplicate_by_id['processed_date']}")
                logger.warning(f"   Workflow: {duplicate_by_id['workflow_path']}")
                logger.warning(f"   OneDrive: {duplicate_by_id['onedrive_link']}")
                
                # Send duplicate notification (optional)
                logger.info("‚è≠Ô∏è Skipping duplicate email processing")
                return  # Early exit!
            
            # Check 2: √Ñhnlicher Content in letzten 24h?
            duplicate_by_content = tracking_db.check_duplicate_by_content(
                subject=subject,
                body=body[:500],  # Nur erste 500 Zeichen
                from_address=from_address,
                hours_window=24
            )
            if duplicate_by_content:
                logger.warning(f"‚ö†Ô∏è DUPLICATE by Content: Similar email from {from_address}")
                logger.warning(f"   Original Subject: {duplicate_by_content['subject']}")
                logger.warning(f"   Original Date: {duplicate_by_content['processed_date']}")
                logger.warning(f"   OneDrive: {duplicate_by_content['onedrive_link']}")
                
                # Save as duplicate reference
                tracking_db.save_email(
                    message_id=message_id,
                    user_email=user_email,
                    from_address=from_address,
                    subject=subject,
                    body=body,
                    received_date=email_data.get("receivedDateTime", ""),
                    workflow_path="DUPLICATE",
                    ai_analysis={},
                    is_duplicate=True,
                    duplicate_of=duplicate_by_content["message_id"]
                )
                
                logger.info("‚è≠Ô∏è Skipping duplicate email processing")
                return  # Early exit!
            
            logger.info("‚úÖ No duplicate found - proceeding with processing")
            
            # ÔøΩüìé PROCESS ATTACHMENTS (if any)
            attachment_results = []
            if len(attachments) > 0:
                logger.info(f"üìé Processing {len(attachments)} attachment(s)...")
                attachment_results = await process_attachments_intelligent(
                    attachments=attachments,
                    message_id=message_id,
                    user_email=user_email,
                    access_token=access_token,
                    subject=subject,
                    document_type_hint=document_type_hint  # üéØ NEW: Pass hint to attachment processing
                )
                logger.info(f"‚úÖ Attachments processed: {len(attachment_results)} results")
                
                # üîç DUPLIKATPR√úFUNG F√úR ATTACHMENTS (File-Hash)
                for att_result in attachment_results:
                    file_bytes = att_result.get("file_bytes")
                    if file_bytes:
                        # Berechne File-Hash
                        file_hash = tracking_db.calculate_file_hash(file_bytes)
                        att_result["file_hash"] = file_hash
                        
                        # Pr√ºfe ob dieser File-Hash bereits existiert
                        duplicate_att = tracking_db.check_duplicate_attachment(file_hash)
                        if duplicate_att:
                            logger.warning(f"‚ö†Ô∏è DUPLICATE ATTACHMENT: {att_result.get('filename')}")
                            logger.warning(f"   Original: {duplicate_att['filename']} from email '{duplicate_att['email_subject']}'")
                            logger.warning(f"   Original Date: {duplicate_att['processed_date']}")
                            logger.warning(f"   OneDrive: {duplicate_att['onedrive_link']}")
                            
                            att_result["is_duplicate"] = True
                            att_result["duplicate_info"] = duplicate_att
                        else:
                            att_result["is_duplicate"] = False
                    else:
                        att_result["file_hash"] = ""
                        att_result["is_duplicate"] = False
            
            # Process with full email data
            result = await orchestrator.process_communication(
                message_type="email",
                from_contact=from_address,
                content=f"{subject}\n\n{body}",
                additional_data={
                    **data,
                    "subject": subject,
                    "body": body,
                    "body_type": body_type,
                    "attachments": attachments,
                    "has_attachments": len(attachments) > 0,
                    "attachments_count": len(attachments),
                    "attachment_results": attachment_results,  # OCR results
                    "document_type_hint": document_type_hint,  # üéØ NEW: Pass hint to AI analysis
                    "priority": priority  # üéØ NEW: Pass priority
                }
            )
            logger.info(f"‚úÖ Email processing complete: {result.get('workflow_path', 'unknown')}")
            
            # ÔøΩ ORDNERSTRUKTUR GENERIEREN
            ai_analysis = result.get("ai_analysis", {})
            
            # Kontext f√ºr folder_logic vorbereiten
            context = {
                "dokumenttyp": ai_analysis.get("dokumenttyp", "unbekannt"),
                "kunde": ai_analysis.get("kunde", "Unbekannt"),
                "lieferant": ai_analysis.get("lieferant", "Unbekannt"),
                "projekt": ai_analysis.get("projektnummer", "Unbekannt"),
                "datum_dokument": ai_analysis.get("datum", ""),
                "valid_attachments": attachment_results  # Alle verarbeiteten Anh√§nge
            }
            
            try:
                folder_info = generate_folder_and_filenames(
                    context=context,
                    gpt_result=ai_analysis,
                    attachments=attachment_results
                )
                
                logger.info(f"üìÅ Ordnerstruktur generiert: {folder_info.get('ordnerstruktur')}")
                logger.info(f"üìÑ Dateinamen ({len(folder_info.get('pdf_filenames', []))}): {folder_info.get('pdf_filenames')}")
                
                # Ordnerstruktur zu attachment_results hinzuf√ºgen
                for i, att_result in enumerate(attachment_results):
                    if i < len(folder_info.get("pdf_filenames", [])):
                        att_result["target_folder"] = folder_info.get("ordnerstruktur")
                        att_result["target_filename"] = folder_info["pdf_filenames"][i]
                        att_result["target_full_path"] = f"{folder_info['ordnerstruktur']}/{folder_info['pdf_filenames'][i]}"
                        logger.info(f"  ‚Üí {att_result.get('filename')} ‚Üí {att_result['target_full_path']}")
                
            except Exception as folder_error:
                logger.error(f"‚ùå Fehler bei Ordnerstruktur-Generierung: {folder_error}")
                # Fallback: Basis-Ordner verwenden
                for att_result in attachment_results:
                    att_result["target_folder"] = "Scan/Unbekannt"
                    att_result["target_filename"] = att_result.get("filename", "unknown.pdf")
                    att_result["target_full_path"] = f"Scan/Unbekannt/{att_result['target_filename']}"
            
            # ‚òÅÔ∏è ONEDRIVE UPLOAD
            logger.info(f"‚òÅÔ∏è Starte OneDrive Upload f√ºr {len(attachment_results)} Dateien...")
            
            for att_result in attachment_results:
                try:
                    # Skip Duplikate
                    if att_result.get("is_duplicate", False):
                        logger.warning(f"‚è≠Ô∏è Skipping duplicate: {att_result.get('filename')}")
                        continue
                    
                    file_bytes = att_result.get("file_bytes")
                    target_folder = att_result.get("target_folder", "Scan/Unbekannt")
                    target_filename = att_result.get("target_filename", att_result.get("filename", "unknown.pdf"))
                    
                    if not file_bytes:
                        logger.warning(f"‚ö†Ô∏è No file_bytes for {att_result.get('filename')}, skipping upload")
                        continue
                    
                    logger.info(f"‚¨ÜÔ∏è Uploading: {target_filename} ‚Üí {target_folder}")
                    
                    # OneDrive Upload
                    upload_url = await upload_file_to_onedrive(
                        user_mail=user_email,
                        folder_path=target_folder,
                        filename=target_filename,
                        file_bytes=file_bytes,
                        access_token_onedrive=access_token
                    )
                    
                    if upload_url:
                        logger.info(f"‚úÖ Upload erfolgreich: {upload_url}")
                        att_result["onedrive_uploaded"] = True
                        att_result["onedrive_path"] = f"{target_folder}/{target_filename}"
                        att_result["onedrive_url"] = upload_url
                        
                        # Update tracking DB
                        try:
                            tracking_db.update_onedrive_upload(
                                message_id=message_id,
                                onedrive_path=att_result["onedrive_path"],
                                onedrive_link=upload_url
                            )
                            logger.info(f"üíæ Tracking DB updated with OneDrive path")
                        except Exception as db_update_error:
                            logger.error(f"‚ùå Failed to update tracking DB: {db_update_error}")
                    else:
                        logger.error(f"‚ùå Upload fehlgeschlagen f√ºr {target_filename}")
                        att_result["onedrive_uploaded"] = False
                        att_result["onedrive_error"] = "Upload failed"
                        
                except Exception as upload_error:
                    logger.error(f"‚ùå Upload-Fehler f√ºr {att_result.get('filename')}: {upload_error}")
                    att_result["onedrive_uploaded"] = False
                    att_result["onedrive_error"] = str(upload_error)
            
            # üíæ SPEICHERN IN EMAIL TRACKING DB
            processing_time = asyncio.get_event_loop().time() - processing_start_time

            
            try:
                email_id = tracking_db.save_email(
                    message_id=message_id,
                    user_email=user_email,
                    from_address=from_address,
                    subject=subject,
                    body=body[:1000],  # Nur erste 1000 Zeichen
                    received_date=email_data.get("receivedDateTime", ""),
                    workflow_path=result.get("workflow_path", "unknown"),
                    ai_analysis=ai_analysis,
                    attachment_results=attachment_results,
                    processing_time=processing_time,
                    is_duplicate=False,
                    duplicate_of=None
                )
                logger.info(f"üíæ Email saved to tracking DB with ID: {email_id}")
            except Exception as db_error:
                logger.error(f"‚ùå Failed to save to tracking DB: {db_error}")
            
        else:
            # Fallback: Process with provided data (no Graph API)
            logger.warning("‚ö†Ô∏è No message_id provided - processing with limited data from Zapier")
            result = await orchestrator.process_communication(
                message_type="email",
                from_contact=data.get("from", ""),
                content=data.get("content", data.get("subject", "")),
                additional_data=data
            )
            logger.info(f"‚úÖ Email processing complete (fallback): {result.get('workflow_path', 'unknown')}")
        
    except Exception as e:
        logger.error(f"‚ùå Background email processing error: {e}")
        import traceback
        logger.error(traceback.format_exc())

@app.post("/webhook/ai-call")
async def process_call(request: Request):
    """
    üìû SIPGATE CALL PROCESSING (SipGate Assist + FrontDesk)
    
    Supports TWO webhook sources:
    1. SipGate Assist API (nested structure: data["call"], data["assist"])
    2. FrontDesk (flat structure with recording_url, etc.)
    
    Flow:
    1. Telefonnummer-Matching in WeClapp (phone-eq=...)
    2. WEG_A: Unbekannt ‚Üí Notification Email mit 4 Buttons
    3. WEG_B: Bekannt ‚Üí CRM Log mit Transcript, AI Analysis, Tasks
    """
    
    try:
        data = await request.json()
        logger.info(f"üìû Call Webhook Payload: {json.dumps(data, ensure_ascii=False)[:1000]}")
        
        # üîç DETECT WEBHOOK SOURCE: SipGate Assist vs FrontDesk
        is_sipgate_assist = "call" in data and "assist" in data
        is_frontdesk = "recording_url" in data or "audio_url" in data or "transcription_url" in data
        
        if is_sipgate_assist:
            logger.info("‚úÖ Detected: SipGate Assist API webhook")
            # üéØ SIPGATE ASSIST API STRUCTURE (v1)
            call_data = data.get("call", {})
            assist_data = data.get("assist", {})
            summary_data = assist_data.get("summary", {})
        elif is_frontdesk:
            logger.info("‚úÖ Detected: FrontDesk webhook")
            # FrontDesk sends flat structure
            call_data = data  # All data at root level
            assist_data = {}
            summary_data = {}
        else:
            logger.warning("‚ö†Ô∏è Unknown webhook format - treating as generic call")
            call_data = data
            assist_data = {}
            summary_data = {}
        
        logger.info(f"‚úÖ Processing call with source: {'SipGate Assist' if is_sipgate_assist else 'FrontDesk' if is_frontdesk else 'Unknown'}")
        
        # Extract call direction (format differs between sources)
        if is_sipgate_assist:
            call_direction = call_data.get("direction", "in")  # "in" or "out"
            call_direction = "inbound" if call_direction == "in" else "outbound"
        else:
            # FrontDesk or generic format
            call_direction = call_data.get("call_direction", call_data.get("direction", "inbound"))
        
        # üéØ EXTRACT CALL INFO (source-aware)
        if is_sipgate_assist:
            call_id = call_data.get("id", "")
            call_duration = call_data.get("duration", 0) // 1000  # SipGate: milliseconds
            call_start_time = call_data.get("startTime", "")
            call_end_time = call_data.get("endTime", "")
            call_users = call_data.get("users", [])
            
            # Phone numbers from nested structure
            if call_direction == "outbound":
                external_number = call_data.get("to", "")
                our_number = call_data.get("from", "")
            else:
                external_number = call_data.get("from", "")
                our_number = call_data.get("to", "")
        else:
            # FrontDesk format (flat structure)
            call_id = call_data.get("call_id", call_data.get("id", ""))
            call_duration = call_data.get("duration", call_data.get("call_duration", 0))
            call_start_time = call_data.get("start_time", call_data.get("startTime", ""))
            call_end_time = call_data.get("end_time", call_data.get("endTime", ""))
            call_users = []
            
            # Phone numbers (various field names)
            external_number = (
                call_data.get("caller") or 
                call_data.get("from") or 
                call_data.get("phone") or
                call_data.get("caller_number") or
                ""
            )
            our_number = (
                call_data.get("called") or
                call_data.get("to") or
                call_data.get("recipient") or
                ""
            )
        
        # Normalize phone format (remove spaces, dashes, brackets)
        phone_normalized = external_number.replace(" ", "").replace("-", "").replace("(", "").replace(")", "")
        
        # üéØ EXTRACT TRANSCRIPTION (source-aware)
        if is_sipgate_assist:
            # SipGate Assist: nested in assist.summary
            call_transcript = summary_data.get("content", "")
            key_points = summary_data.get("keyPoints", [])
            topics = summary_data.get("topics", [])
            
            # Extract SmartAnswers (caller name, company, etc.)
            call_scenarios = assist_data.get("callScenarios", [])
            caller_name = ""
            company_name = ""
            caller_request = ""
            
            for scenario in call_scenarios:
                smart_answers = scenario.get("smartAnswers", [])
                for answer_set in smart_answers:
                    items = answer_set.get("setItems", [])
                    for item in items:
                        question = item.get("question", "").lower()
                        answer = item.get("answer", "")
                        
                        if "name" in question and not caller_name:
                            caller_name = answer
                        elif "firma" in question or "company" in question:
                            company_name = answer
                        elif "anliegen" in question or "anfrage" in question:
                            caller_request = answer
        else:
            # FrontDesk: flat structure
            call_transcript = (
                call_data.get("transcription") or
                call_data.get("transcript") or
                call_data.get("text") or
                call_data.get("content") or
                ""
            )
            key_points = call_data.get("key_points", [])
            topics = call_data.get("topics", [])
            
            # Caller details from flat structure
            caller_name = call_data.get("caller_name", call_data.get("name", ""))
            company_name = call_data.get("company", call_data.get("company_name", ""))
            caller_request = call_data.get("request", call_data.get("purpose", ""))
            
            # Recording URL (FrontDesk specific)
            recording_url = (
                call_data.get("recording_url") or
                call_data.get("audio_url") or
                call_data.get("recordingUrl") or
                ""
            )
        
        # Legacy/additional fields
        if not is_sipgate_assist and recording_url:
            logger.info(f"üéôÔ∏è FrontDesk Recording: {recording_url}")
        
        call_status = ""
        assigned_user = call_users[0] if call_users and is_sipgate_assist else ""
        user_id = ""
        notes = ""
        tags = []
        
        # üìä LOG ALL EXTRACTED DATA
        logger.info(f"üìû Call Details:")
        logger.info(f"   Direction: {call_direction} | Duration: {call_duration}s | Call ID: {call_id}")
        logger.info(f"   üìû External: {phone_normalized} | Our Number: {our_number}")
        if caller_name:
            logger.info(f"   üìõ Caller Name: {caller_name}")
        if company_name:
            logger.info(f"   üè¢ Company: {company_name}")
        if caller_request:
            logger.info(f"   üí¨ Request: {caller_request[:100]}...")
        if assigned_user:
            logger.info(f"   üë§ Assigned to: {assigned_user}")
        if key_points:
            logger.info(f"   üîë Key Points: {len(key_points)} items")
        if topics:
            logger.info(f"   üè∑Ô∏è Topics: {', '.join(topics)}")
        
        # Build comprehensive transcript for AI analysis
        if not call_transcript:
            call_transcript = f"Anruf {call_direction} - Dauer: {call_duration}s - Keine Transkription verf√ºgbar"
        else:
            # Enhance transcript with structured data
            transcript_parts = [call_transcript]
            
            if caller_name:
                transcript_parts.append(f"\nüë§ Anrufer: {caller_name}")
            if company_name:
                transcript_parts.append(f"\nüè¢ Firma: {company_name}")
            if key_points:
                transcript_parts.append(f"\n\nüîë Wichtige Punkte:\n" + "\n".join(f"- {point}" for point in key_points))
            
            call_transcript = "".join(transcript_parts)
        
        # Process through orchestrator (includes phone matching)
        result = await orchestrator.process_communication(
            message_type="call",
            from_contact=phone_normalized,
            content=call_transcript,
            additional_data={
                **data,
                # Source identification
                "webhook_source": "sipgate_assist" if is_sipgate_assist else "frontdesk" if is_frontdesk else "unknown",
                # Core call data
                "call_direction": call_direction,
                "call_duration": call_duration,
                "call_id": call_id,
                "call_start_time": call_start_time,
                "call_end_time": call_end_time,
                "phone_normalized": phone_normalized,
                "external_number": external_number,
                "our_number": our_number,
                # Extracted contact/company data
                "caller_name": caller_name,
                "company_name": company_name,
                "caller_request": caller_request,
                # AI/transcription data
                "key_points": key_points,
                "topics": topics,
                "recording_url": recording_url if 'recording_url' in locals() else "",
                # Legacy fields
                "assigned_user": assigned_user,
                "user_id": user_id,
                "notes": notes,
                "tags": tags
            }
        )
        
        logger.info(f"‚úÖ Call processing complete: {result.get('workflow_path', 'unknown')}")
        
        return {"ai_processing": result}
        
    except Exception as e:
        logger.error(f"‚ùå Call processing error: {str(e)}")
        import traceback
        logger.error(traceback.format_exc())
        # ‚ö†Ô∏è SECURITY: Never expose internal error details to client
        raise HTTPException(status_code=500, detail="Internal server error during call processing")

@app.post("/webhook/frontdesk")
async def process_frontdesk(request: Request):
    """
    üéôÔ∏è FRONTDESK CALL RECORDING & TRANSCRIPTION
    
    Dedicated endpoint for FrontDesk webhooks.
    Simpler than /webhook/ai-call - no auto-detection needed.
    
    Expected FrontDesk payload (flat structure):
    {
        "caller": "+49123456789",
        "transcription": "...",
        "recording_url": "https://...",
        "duration": 120,
        "caller_name": "Max Mustermann",
        "company": "Beispiel GmbH"
    }
    """
    
    try:
        data = await request.json()
        logger.info(f"üéôÔ∏è FrontDesk Webhook: {json.dumps(data, ensure_ascii=False)[:1000]}")
        
        # Extract phone number (various field names)
        phone_number = (
            data.get("caller") or 
            data.get("from") or 
            data.get("phone") or
            data.get("caller_number") or
            ""
        )
        
        # Extract transcription
        transcription = (
            data.get("transcription") or
            data.get("transcript") or
            data.get("text") or
            data.get("content") or
            ""
        )
        
        # Extract additional metadata
        caller_name = data.get("caller_name", data.get("name", ""))
        company = data.get("company", data.get("company_name", ""))
        duration = data.get("duration", data.get("call_duration", 0))
        recording_url = data.get("recording_url", data.get("audio_url", ""))
        
        # Log extracted data
        logger.info(f"üìû FrontDesk Call: {phone_number}")
        if caller_name:
            logger.info(f"   üìõ Name: {caller_name}")
        if company:
            logger.info(f"   üè¢ Company: {company}")
        if duration:
            logger.info(f"   ‚è±Ô∏è Duration: {duration}s")
        if recording_url:
            logger.info(f"   üéôÔ∏è Recording: {recording_url}")
        
        # Build enhanced transcript
        transcript_parts = [transcription] if transcription else ["Keine Transkription verf√ºgbar"]
        
        if caller_name:
            transcript_parts.append(f"\nüë§ Anrufer: {caller_name}")
        if company:
            transcript_parts.append(f"\nüè¢ Firma: {company}")
        
        enhanced_transcript = "".join(transcript_parts)
        
        # Process through orchestrator
        result = await orchestrator.process_communication(
            message_type="call",
            from_contact=phone_number,
            content=enhanced_transcript,
            additional_data={
                **data,
                "webhook_source": "frontdesk",
                "call_direction": data.get("direction", "inbound"),
                "call_duration": duration,
                "phone_normalized": phone_number.replace(" ", "").replace("-", "").replace("(", "").replace(")", ""),
                "caller_name": caller_name,
                "company_name": company,
                "recording_url": recording_url
            }
        )
        
        logger.info(f"‚úÖ FrontDesk call processing complete: {result.get('workflow_path', 'unknown')}")
        
        return {
            "status": "success",
            "message": "FrontDesk call processed",
            "workflow_path": result.get("workflow_path"),
            "phone": phone_number
        }
        
    except Exception as e:
        logger.error(f"‚ùå FrontDesk processing error: {str(e)}")
        import traceback
        logger.error(traceback.format_exc())
        raise HTTPException(status_code=500, detail="Internal server error")

@app.post("/webhook/feedback")
async def process_feedback(request: Request):
    """
    üêõ FEEDBACK & PROBLEM REPORTING
    
    Sammelt Feedback von Mitarbeitern zu:
    - Fehlverhalten des Systems
    - Falsche Kontakt-Zuordnungen
    - Fehlende Funktionen
    - Verbesserungsvorschl√§ge
    - Datenqualit√§t (korrekt/fehlerhaft)
    
    Expected payload:
    {
        "type": "bug|feature|improvement|wrong_match|data_good|data_error",
        "message": "Beschreibung des Problems",
        "context": {
            "email_id": "...",
            "contact_id": "...",
            "workflow_path": "..."
        }
    }
    """
    
    try:
        data = await request.json()
        logger.info(f"üêõ Feedback received: {json.dumps(data, ensure_ascii=False)[:500]}")
        
        feedback_type = data.get("type", "general")
        message = data.get("message", "")
        context = data.get("context", {})
        reporter = data.get("reporter", "unknown")
        
        # Handle special action types from notification buttons
        action = data.get("action", feedback_type)
        
        # Map actions to feedback types
        action_mapping = {
            "data_good": {
                "type": "data_quality_good",
                "default_message": "Alle Daten wurden korrekt ausgewertet und zugeordnet.",
                "emoji": "‚úÖ",
                "priority": "low"
            },
            "data_error": {
                "type": "data_quality_error",
                "default_message": "Fehler bei der Datenauswertung oder -zuordnung erkannt.",
                "emoji": "‚ö†Ô∏è",
                "priority": "high"
            },
            "create_supplier": {
                "type": "supplier_creation",
                "default_message": "Lieferant soll angelegt werden.",
                "emoji": "üè≠",
                "priority": "medium"
            },
            "report_issue": {
                "type": "bug_report",
                "default_message": "Problem oder Fehlverhalten gemeldet.",
                "emoji": "üêõ",
                "priority": "high"
            }
        }
        
        # Get action details or use defaults
        action_info = action_mapping.get(action, {
            "type": feedback_type,
            "default_message": message or "Kein Details angegeben",
            "emoji": "üìù",
            "priority": "medium"
        })
        
        # Use provided message or default
        final_message = message or action_info["default_message"]
        
        # Store in optimization list (simple file-based for now)
        feedback_entry = {
            "timestamp": now_berlin().isoformat(),
            "type": action_info["type"],
            "action": action,
            "message": final_message,
            "context": context,
            "reporter": reporter,
            "priority": action_info["priority"],
            "status": "new"
        }
        
        # Append to feedback log file
        import json as json_lib
        feedback_file = "feedback_log.jsonl"
        try:
            with open(feedback_file, "a") as f:
                f.write(json_lib.dumps(feedback_entry, ensure_ascii=False) + "\n")
            logger.info(f"‚úÖ Feedback stored in {feedback_file}")
        except Exception as e:
            logger.error(f"‚ùå Failed to write feedback: {e}")
        
        # Log to console for immediate visibility
        emoji = action_info["emoji"]
        logger.info(f"{emoji} FEEDBACK [{action_info['type']}] Priority: {action_info['priority'].upper()}: {final_message}")
        if context:
            logger.info(f"   Context: {context}")
        
        # Return user-friendly response based on action
        response_messages = {
            "data_good": "‚úÖ Danke f√ºr die Best√§tigung! Datenqualit√§t dokumentiert.",
            "data_error": "‚ö†Ô∏è Fehler dokumentiert. Wir werden das √ºberpr√ºfen.",
            "create_supplier": "üè≠ Lieferanten-Anfrage erfasst.",
            "report_issue": "üêõ Problem erfasst. Vielen Dank f√ºr die Meldung!"
        }
        
        return {
            "status": "success",
            "message": response_messages.get(action, "Feedback erfasst - Vielen Dank!"),
            "feedback_id": f"fb-{int(now_berlin().timestamp())}",
            "priority": action_info["priority"]
        }
        
    except Exception as e:
        logger.error(f"‚ùå Feedback processing error: {str(e)}")
        raise HTTPException(status_code=500, detail="Internal server error")

@app.post("/webhook/ai-whatsapp")
async def process_whatsapp(request: Request):
    """Process incoming WhatsApp message via webhook"""
    
    try:
        data = await request.json()
        
        result = await orchestrator.process_communication(
            message_type="whatsapp",
            from_contact=data.get("from", ""),
            content=data.get("message", data.get("content", "")),
            additional_data=data
        )
        
        return {"ai_processing": result}
        
    except Exception as e:
        logger.error(f"WhatsApp processing error: {e}")
        # ‚ö†Ô∏è SECURITY: Never expose internal error details to client
        raise HTTPException(status_code=500, detail="Internal server error during WhatsApp processing")

@app.api_route("/webhook/contact-action", methods=["GET", "POST"])
async def handle_contact_action(request: Request):
    """
    üéØ MITARBEITER-AKTIONEN f√ºr unbekannte Kontakte
    
    Empf√§ngt Entscheidung des Mitarbeiters und f√ºhrt entsprechende WeClapp API Calls aus:
    - create_contact: POST /party (neuer Kontakt)
    - mark_private: Custom Attribute "private_contact"
    - mark_spam: Custom Attribute "spam_contact"
    - request_info: POST /crmEvent (R√ºckfrage dokumentieren)
    
    Akzeptiert GET (Query Params) und POST (JSON Body)
    """
    
    try:
        # Support both GET and POST
        if request.method == "GET":
            # GET: Extract from query parameters
            action = request.query_params.get("action")
            contact_email = request.query_params.get("sender")  # From email link
            email_id = request.query_params.get("email_id")
            contact_data = {}  # Optional: Could parse from query params
        else:
            # POST: Extract from JSON body
            data = await request.json()
            action = data.get("action")
            contact_email = data.get("contact_email") or data.get("sender")
            email_id = data.get("email_id")
            contact_data = data.get("contact_data", {})
        
        # üÜï FALLBACK: If sender is empty, try to get it from email database
        sender_name = None
        if not contact_email and email_id:
            logger.info(f"‚ö†Ô∏è Sender empty, looking up in database for email_id: {email_id}")
            try:
                db_conn = sqlite3.connect(DB_PATH)
                cursor = db_conn.execute(
                    "SELECT sender, sender_name FROM email_data WHERE id = ? OR subject LIKE ?",
                    (email_id, f"%{email_id}%")
                )
                row = cursor.fetchone()
                db_conn.close()
                if row:
                    contact_email = row[0]
                    sender_name = row[1] if len(row) > 1 else None
                    logger.info(f"‚úÖ Found sender from database: {contact_email} ({sender_name})")
                else:
                    logger.warning(f"‚ùå No email found in database for email_id: {email_id}")
            except Exception as db_error:
                logger.error(f"‚ùå Database lookup error: {str(db_error)}")
        
        if not action or not contact_email:
            raise HTTPException(status_code=400, detail="action and contact_email/sender required")
        
        logger.info(f"üìã Contact Action received: {action} for {contact_email}")
        
        result = {}
        
        # ACTION 1: Kontakt im CRM anlegen
        if action == "create_contact":
            async with aiohttp.ClientSession() as session:
                headers = {
                    "AuthenticationToken": orchestrator.weclapp_api_token,
                    "Content-Type": "application/json",
                    "Accept": "application/json"
                }
                
                # üîß Namen-Parsing: Versuche sender_name zu splitten oder nutze Placeholder
                first_name = contact_data.get("first_name", "")
                last_name = contact_data.get("last_name", "")
                
                # Fallback 1: sender_name aus DB (wenn vorhanden)
                if not first_name and not last_name and sender_name:
                    name_parts = sender_name.strip().split(maxsplit=1)
                    first_name = name_parts[0] if len(name_parts) > 0 else "Unbekannt"
                    last_name = name_parts[1] if len(name_parts) > 1 else "Kontakt"
                
                # üéØ CHECK: Is contact_email actually a PHONE NUMBER?
                is_phone = contact_email and (contact_email.startswith("+") or contact_email.replace(" ", "").isdigit())
                
                # Fallback 2: Phone Number ‚Üí Generate dummy email + use number as name
                if is_phone:
                    phone_clean = contact_email.replace(" ", "").replace("-", "").replace("(", "").replace(")", "")
                    first_name = f"Tel {phone_clean}"
                    last_name = "Kontakt"
                    # Generate dummy email: phone@noemail.local
                    dummy_email = f"{phone_clean}@noemail.local"
                    phone_number = phone_clean
                elif not first_name and contact_email:
                    # Fallback 3: Email-Prefix als Vorname (z.B. "jaszczyk" ‚Üí "Jaszczyk")
                    if "@" in contact_email:
                        email_prefix = contact_email.split("@")[0]
                        first_name = email_prefix.capitalize()
                        last_name = "Kontakt"
                        dummy_email = contact_email
                        phone_number = ""
                    else:
                        # Invalid format - use placeholder
                        first_name = "Unbekannt"
                        last_name = "Kontakt"
                        dummy_email = f"unknown{contact_email}@noemail.local"
                        phone_number = ""
                else:
                    # Fallback 4: Absolute Placeholders
                    if not first_name:
                        first_name = "Unbekannt"
                        last_name = "Kontakt"
                    dummy_email = contact_email if "@" in contact_email else f"unknown@noemail.local"
                    phone_number = ""
                
                party_data = {
                    "partyType": "PERSON",
                    "email": dummy_email,  # Always valid email format!
                    "firstName": first_name,
                    "lastName": last_name,
                    "company": contact_data.get("company", ""),
                    "phone": phone_number or contact_data.get("phone", "") or (contact_email if is_phone else ""),
                    "tags": ["AI_GENERATED", "UNKNOWN_CONTACT_CONVERTED"]
                }
                
                url = f"https://{orchestrator.weclapp_domain}.weclapp.com/webapp/api/v2/party"
                
                async with session.post(url, headers=headers, json=party_data) as response:
                    if response.status == 201:
                        created_party = await response.json()
                        result = {
                            "success": True,
                            "action": "contact_created",
                            "party_id": created_party.get("id"),
                            "message": f"Kontakt {contact_email} erfolgreich angelegt"
                        }
                        logger.info(f"‚úÖ Contact created: {created_party.get('id')}")
                    else:
                        result = {
                            "success": False,
                            "error": f"WeClapp API error: {response.status}"
                        }
        
        # ACTION 2: Als privat markieren
        elif action == "mark_private":
            # Erstelle CRM Event f√ºr Dokumentation
            async with aiohttp.ClientSession() as session:
                headers = {
                    "AuthenticationToken": orchestrator.weclapp_api_token,
                    "Content-Type": "application/json"
                }
                
                crm_event = {
                    "type": "NOTE",
                    "description": f"Kontakt als PRIVAT markiert: {contact_email}",
                    "tags": ["PRIVATE_CONTACT"]
                }
                
                url = f"https://{orchestrator.weclapp_domain}.weclapp.com/webapp/api/v2/crmEvent"
                
                async with session.post(url, headers=headers, json=crm_event) as response:
                    result = {
                        "success": response.status == 201,
                        "action": "marked_private",
                        "message": f"{contact_email} als privat markiert"
                    }
        
        # ACTION 3: Als Spam markieren
        elif action == "mark_spam":
            async with aiohttp.ClientSession() as session:
                headers = {
                    "AuthenticationToken": orchestrator.weclapp_api_token,
                    "Content-Type": "application/json"
                }
                
                crm_event = {
                    "type": "NOTE",
                    "description": f"Kontakt als SPAM markiert: {contact_email}",
                    "tags": ["SPAM_CONTACT", "BLACKLIST"]
                }
                
                url = f"https://{orchestrator.weclapp_domain}.weclapp.com/webapp/api/v2/crmEvent"
                
                async with session.post(url, headers=headers, json=crm_event) as response:
                    result = {
                        "success": response.status == 201,
                        "action": "marked_spam",
                        "message": f"{contact_email} als Spam markiert"
                    }
        
        # ACTION 4: Weitere Informationen einholen
        elif action == "request_info":
            async with aiohttp.ClientSession() as session:
                headers = {
                    "AuthenticationToken": orchestrator.weclapp_api_token,
                    "Content-Type": "application/json"
                }
                
                task_data = {
                    "title": f"Weitere Infos einholen: {contact_email}",
                    "description": f"Kontakt {contact_email} - Zus√§tzliche Informationen anfordern vor CRM-Aufnahme",
                    "status": "OPEN",
                    "priority": "MEDIUM",
                    "dueDate": (now_berlin() + timedelta(days=2)).isoformat()
                }
                
                url = f"https://{orchestrator.weclapp_domain}.weclapp.com/webapp/api/v2/task"
                
                async with session.post(url, headers=headers, json=task_data) as response:
                    if response.status == 201:
                        task = await response.json()
                        result = {
                            "success": True,
                            "action": "info_requested",
                            "task_id": task.get("id"),
                            "message": f"Follow-up Task erstellt f√ºr {contact_email}"
                        }
                    else:
                        result = {"success": False, "error": f"Task creation failed: {response.status}"}
        
        else:
            raise HTTPException(status_code=400, detail=f"Unknown action: {action}")
        
        return result
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"‚ùå Contact action error: {e}")
        raise HTTPException(status_code=500, detail="Internal server error")

@app.get("/status")
async def system_status():
    """System status and configuration"""
    
    return {
        "system": "AI Communication Orchestrator",
        "framework": "LangGraph + LangChain + FastAPI",
        "status": "ONLINE",
        "configuration": {
            "openai_configured": bool(orchestrator.openai_api_key),
            "weclapp_configured": bool(orchestrator.weclapp_api_token),
            "apify_configured": bool(orchestrator.apify_token)
        },
        "workflow_nodes": [
            "contact_lookup",
            "ai_analysis", 
            "workflow_routing",
            "weg_a_unknown_contact",
            "weg_b_known_contact",
            "finalize_processing"
        ]
    }

@app.post("/admin/cache/reset")
async def reset_contact_cache(request: Request):
    """üóëÔ∏è ADMIN: Reset Email Database Cache"""
    
    try:
        # Optional: Add authentication here
        # auth_header = request.headers.get("Authorization")
        # if auth_header != f"Bearer {os.getenv('ADMIN_TOKEN')}":
        #     raise HTTPException(status_code=401, detail="Unauthorized")
        
        conn = sqlite3.connect(DB_PATH)
        cursor = conn.cursor()
        
        # Get cache stats before reset
        cursor.execute("SELECT COUNT(*) FROM email_data")
        total_entries = cursor.fetchone()[0] or 0
        
        # Clear cache
        cursor.execute("DELETE FROM email_data")
        conn.commit()
        conn.close()
        
        logger.warning(f"‚ö†Ô∏è EMAIL DATABASE RESET: Deleted {total_entries} entries")
        
        return {
            "status": "success",
            "message": "Email database reset successfully",
            "deleted_entries": total_entries
        }
        
    except Exception as e:
        logger.error(f"‚ùå Cache reset error: {e}")
        raise HTTPException(status_code=500, detail=f"Cache reset failed: {str(e)}")

# ===============================
# PRODUCTION SERVER STARTUP
# ===============================

if __name__ == "__main__":
    print("üöÄ STARTING PRODUCTION AI ORCHESTRATOR")
    print("=" * 50)
    print("‚úÖ LangGraph Workflow initialized")
    print("‚úÖ FastAPI server configured")
    print("‚úÖ OpenAI GPT-4 integration ready")
    print("‚úÖ WeClapp CRM integration ready")
    print("üåê Server starting on http://0.0.0.0:5001")
    print("")
    print("üì° WEBHOOK ENDPOINTS:")
    print("  - POST /webhook/ai-email")
    print("  - POST /webhook/ai-call")
    print("  - POST /webhook/frontdesk  üéôÔ∏è")
    print("  - POST /webhook/feedback   üêõ NEW")
    print("  - POST /webhook/ai-whatsapp")
    print("")
    print("üéØ READY FOR PRODUCTION DEPLOYMENT!")
    
    # Production server
    uvicorn.run(
        "production_langgraph_orchestrator:app",
        host="0.0.0.0",
        port=5001,
        reload=False,  # Production mode
        workers=1,
        log_level="info"
    )